%% Version 4.3.2, 25 August 2014
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template.tex --  LaTeX-based template for submissions to the 
% American Meteorological Society
%
% Template developed by Amy Hendrickson, 2013, TeXnology Inc., 
% amyh@texnology.com, http://www.texnology.com
% following earlier work by Brian Papa, American Meteorological Society
%
% Email questions to latex@ametsoc.org.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Start with one of the following:
% DOUBLE-SPACED VERSION FOR SUBMISSION TO THE AMS
%\documentclass{ametsoc}


% TWO-COLUMN JOURNAL PAGE LAYOUT---FOR AUTHOR USE ONLY
\documentclass[twocol]{ametsoc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% To be entered only if twocol option is used

\journal{jcli}

%  Please choose a journal abbreviation to use above from the following list:
% 
%   jamc     (Journal of Applied Meteorology and Climatology)
%   jtech     (Journal of Atmospheric and Oceanic Technology)
%   jhm      (Journal of Hydrometeorology)
%   jpo     (Journal of Physical Oceanography)
%   jas      (Journal of Atmospheric Sciences)	
%   jcli      (Journal of Climate)
%   mwr      (Monthly Weather Review)
%   wcas      (Weather, Climate, and Society)
%   waf       (Weather and Forecasting)
%   bams (Bulletin of the American Meteorological Society)
%   ei    (Earth Interactions)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Citations should be of the form ``author year''  not ``author, year''
\bibpunct{(}{)}{;}{a}{}{,}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% To be entered by author:

%% May use \\ to break lines in title:

\title{Global Optimization of the Analogue Method by Means of Genetic Algorithms}

%%% Enter authors' names, as you see in this example:
%%% Use \correspondingauthor{} and \thanks{Current Affiliation:...}
%%% immediately following the appropriate author.
%%%
%%% Note that the \correspondingauthor{} command is NECESSARY.
%%% The \thanks{} commands are OPTIONAL.

    %\authors{Author One\correspondingauthor{Author One, 
    % American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}
% and Author Two\thanks{Current affiliation: American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}}

\authors{Pascal Horton\correspondingauthor{Terranum, Rue de l'industrie 35 bis, 1030 Bussigny, Switzerland.}}

%% Follow this form:
    % \affiliation{American Meteorological Society, 
    % Boston, Massachusetts.}

\affiliation{University of Lausanne and Terranum, Lausanne, Switzerland}

%% Follow this form:
    %\email{latex@ametsoc.org}

\email{pascal.horton@alumnil.unil.ch}

%% If appropriate, add additional authors, different affiliations:
    %\extraauthor{Extra Author}
    %\extraaffil{Affiliation, City, State/Province, Country}

\extraauthor{Michel Jaboyedoff}
\extraaffil{University of Lausanne, Lausanne, Switzerland}

%% May repeat for a additional authors/affiliations:

%\extraauthor{}
%\extraaffil{}

\extraauthor{Charles Obled}
\extraaffil{Universit\'{e} de Grenoble-Alpes, LTHE, Grenoble, France}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%
% Enter your abstract here
% Abstracts should not exceed 250 words in length!
%
% For BAMS authors only: If your article requires a Capsule Summary, please place the capsule text at the end of your abstract
% and identify it as the capsule. Example: This is the end of the abstract. (Capsule Summary) This is the capsule summary. 

\abstract{The analogues method is based on a statistical relationship between synoptic atmospheric variables (predictors) and local weather (predictand), which we aim at forecasting. This relationship is expressed through many parameters that are usually calibrated by means of a semi-automatic procedure. This sequential calibration has strong limitations: it is made of successive steps and thus cannot handle parameters dependencies, and it cannot automatically optimize some parameters, such as the selection of the pressure levels and the temporal windows on which the predictors are compared.
In order to surpass these limitations, a global optimization technique was assessed, namely genetic algorithms, which can optimize jointly all parameters of the method and get closer to a global optimum by taking into account the parameters dependencies. Moreover, it can choose objectively parameters that were previously manually assessed, and can take into account new degrees of freedom that were unthinkable before.
These kind of optimization techniques need however to be tailored to the problem to solve. We had to assess multiple combinations of algorithms, and even to develop new operators, such as the \textit{chromosome of adaptive search radius} which was found to be very robust, in order to make recommendations for the use of genetic algorithms to optimize the analogue method. These recommendations are a consequent outcome of this work, as it opens new perspective for the improvement of the analogue method, and its application to new regions or to new predictands.
}

\begin{document}

%% Necessary!
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MAIN BODY OF PAPER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%% In all cases, if there is only one entry of this type within
%% the higher level heading, use the star form: 
%%
% \section{Section title}
% \subsection*{subsection}
% text...
% \section{Section title}

%vs

% \section{Section title}
% \subsection{subsection one}
% text...
% \subsection{subsection two}
% \section{Section title}

%%%
% \section{First primary heading}

% \subsection{First secondary heading}

% \subsubsection{First tertiary heading}

% \paragraph{First quaternary heading}


\section{Introduction}
\label{section_intro}

The analogue method (AM) relies on the hypothesis that similar situations in terms of atmospheric circulation are likely to lead to similar local weather \citep{Lorenz1956, Lorenz1969, Duband1970, Bontron2005}. The principle is sampling a certain number of past situations based on different atmospheric variables (predictors) in order to build a probabilistic forecast for a local weather variable (predictand). The most common usage of the AM is for precipitation forecasting \citep[eg.]{Guilbaud1997, Bontron2005, Bliefernicht2010, Marty2012, Horton2012, Radanovics2013, BenDaoud2015}, but AMs, or equivalent, were also used for short to medium term forecasting of daily temperatures \citep{Radinovic1975, Woodcock1980, Kruizinga1983}, wind \citep{Gordon1987}, snow avalanches \citep{Obled1980, Bolognesi1993}, insolation \citep{Bois1981}, and the trajectory of tropical cyclones \citep{Keenan1981, Sievers2000, Fraedrich2003}. Applications for monthly forecasts in many countries also exist, including Canada \citep{Shabbar1986},  Hungary \citep{Toth1989}, the Netherlands \citep{Nap1981}, and England \citep{Murray1974}, as well as seasonal forecasts: \citet{Barnett1978}, \citet{Bergen1982} and \citet{Livezey1988}.

The purpose of this paper is not to present in details the AM and its different parametrizations, which can be found in \citet{Horton2016} or \citet{BenDaoud2015}. The concepts one needs to understand here are the following: the AM consists in searching a certain number of past situations in a meteorological archive (typically reanalysis) that are the most similar, according to an analogy criteria, and to extract the observed values of the local weather variable of interest from another archive (typically local time series) in order to build the cumulative distribution considered as the probabilistic forecast for the target day. Predictors can be multiple, for example the geopotential height at different pressure levels on different temporal windows (time of observation). The method is usually made of multiple levels of analogy, leading to successive subsampling on predictors of different nature (eg. atmospheric circulation, moisture variables, vertical motion, and air temperature).

Even though the method is rather simple compared to numerical weather prediction models, it contains a certain number of parameters one needs to determine, such as the choice of the predictor variable, its pressure level and temporal window to consider, the spatial domain to use for the comparison, as well as the analogy criteria itself, and finally the number of analogue situations to keep at each subsampling level. Up to now, the optimization of the method has been undertaken by semi-automatic sequential calibration procedures \citep[see][for its definition]{Bontron2004, Horton2016}. However, the choice of predictor variables, pressure levels and temporal windows were still to be chosen manually before optimizing the spatial windows and the number of analogues. Testing multiple combinations of these can be cumbersome, especially when considering multiple predictors within the same level of analogy. Moreover, the sequential technique proceeds to the optimization sequentially, ignoring potential dependencies between the parameters of the method, may they be within a single level of analogy or between them. Thus, optimizing the method with the sequential technique is laborious, as many combinations of predictors (variables, pressure levels, temporal windows) have to be assessed. Moreover, due to its sequential approach, the risk of ending in a local optimum is high and can not be avoided.

In order to overcome these limitations, two optimization techniques were assessed. First, \citet{Horton2012a} assessed the ability of the \citet{Nelder1965a} method based on a simplex approach. This technique didn't provide satisfying results and failed at identifying the global optimum. Indeed, the results didn't converge and many local optimums came out. The parameter space of the AM is very complex and not appropriate for a linear optimization technique. The conclusion was that global optimization techniques were necessary in order to calibrate AMs. The relevance of Genetic Algorithms (GAs) is presented here, which doesn't exclude that other global optimization techniques could also be successful. 

This article is not about discussing the details of the results of an optimization with GAs, but describing how to use GAs in order to successfully optimize AMs. Indeed, GAs variants are numerous and they always need to be tailored to the problem to be addressed. This requires intensive and systematic comparisons of operators and options in order to identify the key factors leading the optimization and the respective sensitivity of the options. These analyses are presented here and result in recommendations for the use of GAs when applied to AMs. The demonstration of the benefit brought by such an approach in a specific case study is the topic of \cite{Horton2016b}. 

We will begin with a motivation for a global optimization (section \ref{sec:motivation}) and then present the concepts of GAs as well as the assessed operators (section \ref{sec:gas}). The comparative analyses of the operators and their results are presented in section \ref{sec:assessment}, which lead us to the recommendations in section \ref{sec:recommendations}.


\section{Motivation for a global optimization}
\label{sec:motivation}

The sequential calibration is fast to optimize one spatial window for a given pressure level and temporal window. However, it doesn't provide an objective selection of the pressure level and temporal window. The only option is to try systematically every combination, which becomes cumbersome already with 2 pressure levels, whereas it seems profitable to consider even more \citep{Horton2012a}.

We observed during the AM calibration that the resulting parameters vary with the initial choices (such as the number of analogues). In addition, the different levels of analogy (eg. on the atmospheric circulation and the moisture index) are always calibrated sequentially. However, we cannot exclude any dependency between them, which could lead us to select another parametrization if we calibrate them together. Simultaneous calibration of all parameters has never been undertaken so far. Only a global optimization technique can be able to optimize all parameters of all analogy levels simultaneously.

When creating the sequential calibration procedure, \citet{Bontron2004} was aware of the problem of dependencies between parameters and wrote: '' \textit{We perceive here the combinatorial aspect of our problem: variables and spatial windows are not independent. We will present our results by first searching the best variable [note: e.g. selection of the pressure level and the temporal window for the geopotential height] on a chosen spatial window, and next, the best window for the chosen variable. However, even by repeating the process, are we sure to obtain the optimal combination?} ''. And later in his work: '' \textit{Our approach, which is again to vary the parameters one by one -- the others being fixed in a more or less arbitrary manner -- may therefore not exactly lead us to the optimal solution} ''. \citet{Bliefernicht2010} has also faced the combinatorial issue of the parameters of the AM and concludes that one needs to be an expert to know their respective influence, their sensitivity and their nonlinear interactions. \citet{BenDaoud2010}, when calibrating the AM, also stated that '' \textit{the combinatory aspect related to the calibration was found to be too high for all the parameters to be calibrated simultaneously } ''.

The AM needs to be adapted to every new region it is applied, because the leading meteorological influences are specific to this region. Even the selection of the pressure levels and the temporal windows should be reconsidered, when not the variable itself. For example, \citet{BenDaoud2010} found the vertical velocity relevant for the great plains in France, when \citet{Horton2012a} found no interest in this variable in an Alpine environment, because the vertical motion, which is mainly controlled by orographic effects, was already well related to the atmospheric circulation itself. So, when adapting the AM to a new region, one should assess systematically every combination of pressure levels, temporal and spatial windows, which is an intensive task. This procedure can be automatized by a global optimization technique.

Our ambition was then to assess the feasibility of an automatic optimization of the AM in order to assess all parameters simultaneously, and thus to be able to get nearer to the global optimum in the parameters space. In addition, it can overcome the systematic manual assessments of all pressure levels and temporal windows. Finally, it can open new perspectives by allowing the addition of new degrees of freedom, such as a weighting of the criteria values between the pressure levels \cite[see]{Horton2016b}, and the consideration of non-overlapping spatial windows between the pressure levels.


\section{Assessed Genetic Algorithms variants}
\label{sec:gas}

Genetic Algorithms (GAs) come from the world of stochastic optimization, more specifically from metaheuristic approaches. These are stochastic iterative algorithms that behave like search algorithms by exploiting the characteristics of a problem and are particularly suitable for complex parameter spaces.

GAs are part of the family of evolutionary algorithms \citep{Back1993b, Schwefel1993}, which get inspiration from some mechanisms of biological evolution, such as reproduction, genetic mutations, chromosomal crossovers, and natural selection. GAs are the most used technique from evolutionary algorithms \citep{Back1993b}, and they are constantly improving \citep{Haupt2004}. However, with time, the different methods of evolutionary algorithms tend to be similar and share many commonalities \citep{Back1996b, Haupt2004}.

The method was originally developed by \citet{Holland1992b} and popularized by \citet{Goldberg1989}. Unlike a linear or local optimization, GAs seek the global optimum on a complex surface, theoretically without restriction, but with no guarantee to reach it.


\subsection{Basic concepts of the genetic algorithms}

GAs mimic the evolution of a population of individuals in a new environment, by applying rules based on natural processes, such as DNA mutation, chromosomes crossover, natural selection, etc. It simulates the fact that in a natural environment, the most suitable individuals tend to survive longer, to reproduce more easily, and so to influence coming generations by providing genes that provide some good performance in a certain domain. Generation after generation, the DNA mixes and the strong genes cumulate in some individuals \citep{Beasley1996a}. Globally, the fitness of the population to its environment increases, while retaining enough variety to not converge too quickly to a local optimum.

Applications of GAs are very diversified as they can handle many parameters of various types \citep{Joines1996a}, even with very complex cost surfaces \citep{Haupt2004}. The objective function to optimize (often named fitness function in this context) can be of different types (mathematical function, experimental or numerical modeling). Only the resulting value is used for optimization. Indeed, these algorithms do not require any knowledge of the problem, which can be used as black-box, but they must be adapted in order to perform optimally.

Two conditions guarantee in theory the convergence to the global optimum \citep{Zitzler2004a}: (1) Parameters mutations that can allow to explore the entire parameter space, thereby ensures that any value can be achieved with a non-zero probability. (2) A rule of elitism ensuring that an optimal solution cannot be lost or damaged.

A key element of the parametrization of GAs is finding the right balance between exploration and exploitation \citep{Back1992a, Smith1997a}. Exploration is characterized by a relatively high probability to assess the regions of the parameter space that have not yet been visited. This probability must be sufficiently large at the beginning of the optimization, so that the algorithm is capable of identifying the region where the global optimum is located. Exploitation is characterized by a local search in an area of interest, and generally makes small movements. The latter is interesting to refine the results at the end of the optimization.


\subsection{Structure and operators}

The GAs optimize a population of $N$ individuals (parameter sets). Each individual contains a chromosome (parameters of the AM in our case). We call gene every parameter that constitutes the chromosome. The floating-point representation \citep[instead of the canonical binary, see][]{Goldberg1989, Goldberg1990a, Holland1992b} of the genes was considered, as it was found more suited in multiple applications \citep{Michalewicz1996, Herrera1998a, Haupt2004, Back1996b, Gaffney2010a}. 

There are numerous implementation variants of GAs often optimal for a given problem \citep{Hart1991a, Schraudolph1992a}. The divergences are the operators implementation, through significantly different algorithms, which has an important effect on the results \citep{Gaffney2010a}. However, the structure of the method (Figure \ref{fig:structure_gas}) resulting from the work of \citet{Holland1992b} is common to most applications \citep{Back1993b}, and consists in the following steps:

\begin{enumerate}
	\item A population of $N$ individuals (parameter sets of the AM) is randomly generated, which constitutes the initial population.
	\item The fitness (performance score or objective function) of every individual is assessed.
	\item A natural selection is applied, after which only the best individuals remain, which constitutes the intermediate generation (IG).
	\item From this IG, couples are formed according to given rules.
	\item These couples proceed to reproduction, or chromosome crossover, in order to mix their genes (parameters) according to the operator version. New children are generated in order to fill the IG back to $N$ individuals.
	\item Parents and children are then subject to mutation, where some genes get affected, meaning some parameter values are randomly changed.
	\item The new formed generation is then re-assessed.
	\item According to the ending criteria, the optimization ends or starts again for another iteration.
\end{enumerate}

\begin{figure}[htb]
	\noindent\includegraphics[width=19pc,angle=0]{figures/figure_structure_gas.pdf}\\
	\caption{Genetic algorithms operational flowchart}
	\label{fig:structure_gas}
\end{figure}

All operators we used and their options, applied to floating-point coding, are described in the following sections. Many other operators exist, but we will only present the ones we evaluated.

\subsubsection{Genesis of the population}

The first step of the optimization is to generate the initial population. A population is a set of $N$ individuals (each of which represents a point in the space of potential solutions, a parameter set of the AM in our application) that we are going to make evolve. A generation is the population at a given time. 

A random initialization based on a uniform distribution is the most current version. The size $N$ of the population is often a compromise between the computation time and the quality of the solution. $N$ must allow sufficient sampling of the solutions field \citep{Beasley1996a}, and should thus vary as a function of chromosome size (ie the number of parameters to be optimized). 


\subsubsection{Natural selection}

Natural selection is performed on the basis of the values of the objective function. The selection allows to only keep a certain part of the population, usually half ($N/2$), which can access the IG (with $N_{mp}$ members). If $N_{mp}$ is too high, the reproduction rate is too low, whereas if it is too small, the strong traits of individuals do not have the ability to accumulate in the same chromosome \citep{Haupt2004}. Several techniques exist, such as:

\begin{itemize}
	\item $N_{mp}$-elitism \citep{Michalewicz1996}: the population is sorted according to the value of the objective function and only the better half is preserved. 
	
	\item Tournament selection \citep{Michalewicz1996, Zitzler2004a}: two individuals are randomly selected and fight. The one with the highest performance score is chosen, but with a certain probability, in order to reduce the selection pressure. This procedure is repeated until the IG is full. Individuals can be selected several times, and thus be represented several times in the IG.
\end{itemize}


\subsubsection{Selection of the couples}

Individuals of the IG can reproduce. It begins with the selection of pairs (the parents). The techniques implemented in this work are the following:


\begin{itemize}
	\item Rank pairing: individuals are gathered in pairs according to their rank (classified on the performance scores). Consecutive ranks are put together (odd rows are associated with even rows). This approach is easy to achieve, but does not look like a natural process.
	
	\item Random pairing: two individuals are randomly selected to form a couple, according to a uniform law.
	
	\item Roulette wheel weighting: this technique refers to gambling, but with a biased roulette. Each individual is associated with a sector of the wheel with a certain opening angle, which is its probability of selection \citep{Haupt2004}. The probability assigned to the individuals is proportional to their fitness (objective function), so that the most adapted individuals have the greatest probability of reproduction. There are two techniques for weighting the individuals of the IG:
	
	\textit{Roulette wheel weighting on rank}: the probability of each individual depends on its rank $n$:
	\begin{equation}
	p_{n}=\dfrac{N_{mp}-n+1}{\sum^{N_{mp}}_{n=1}n}
	\label{equation_mating_rank_weighting}
	\end{equation}
	
	\textit{Roulette wheel weighting on fitness}: the selection probability is calculated based on the value of the objective function. The probability $p_{n}$ of each individual is calculated by the equation \ref{equation_mating_score_weighting}:
	\begin{equation}
	p_{n}=\frac{score_{n}-score_{N_{mp}}}{\sum_{n=1}^{N_{mp}} (score_{n}-score_{N_{mp}})}
	\label{equation_mating_score_weighting}
	\end{equation}
	In our application, the last individual ($N_{mp}$) has zero probability of being selected.
	
	
	\item Tournament selection: This operator is similar to the one used in natural selection, but is applied here for the successive selection of each parent. To select a parent, a number of individuals (2 or 3) are randomly picked and the best is kept. This operation is performed twice, once for each partner. This approach imitates the breeding competition in nature \citep{Haupt2004}.
\end{itemize}


\subsubsection{Chromosome crossover}

Once the two parents are selected for breeding, they combine their chromosomes and produce two children, bringing the number of individuals in the population back to $N$ (the parents also return back in the total population in order to complement the next generation). The combination of chromosomes is carried out using a crossover operator, thereby generating two offspring having characteristics derived from both parents. Chromosome crossover widens the search space and favours the combination of strong genes, which can result in more suited children. It allows a mixing of genes and accumulation of positive mutations.

The evaluated crossover operators are the following:

\begin{itemize}
	\item Single-point crossover: a crossover point is randomly chosen for the pair. The genes (our parameters) located after that point are exchanged in between the two chromosomes.
	
	\item Two-point crossover: works like the single-point crossover, but there are two intersections defining the segments to exchange. This approach, which significantly extends the search space for the children, is considered more efficient than the previous \citep{Beasley1993a}.
	
	\item Multiple-point crossover \citep{DeJong1975a}: it is a generalization of the previous, with a number of crossover points up to the number of genes.
	
	\item Uniform crossover \citep{Syswerda1989}: for each gene of the chromosome, it is randomly chosen to exchange or not the values between the parents.
	
	\item Binary-like crossover \citep{Haupt2004}: chromosome crossover on a binary coding can generate new values for variables located at intersection points, since the crossovers are applied at the bit level, thus often within a gene. This is not the case for the floating-point representation, since the crossover is performed between the genes. To reproduce the behaviour present in the original algorithms, which introduces new information, \citet{Haupt2004} propose an operator that combines standard crossover with an interpolation approach. The genes located after a crossover point are exchanged, but the gene located at the intersection is modified as follows (equation \ref{equation_mating_as_binary}):
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = g_{p1,n} - \beta (g_{p1,n} - g_{p2,n}) \\
	g_{o2,n} = g_{p2,n} + \beta (g_{p1,n} - g_{p2,n}) \\
	\end{array} \right.
	\label{equation_mating_as_binary}
	\end{equation}
	where $g_{o1,n}$ and $g_{o2,n}$ are the $n$-th gene of the two new offspring, and $g_{p1,n}$ and $g_{p2,n}$ are those of the two parents. $\beta$ is a random value between 0 and 1.
	
	\item Blending method \citep{Radcliffe1991a}: in this approach, instead of exchanging the genes in between the chromosomes after one or multiple crossover points, these are combined by linear combination (equation \ref{equation_mating_blending_method}). The genes of the parents are blended together using a random value ($\beta$) that can be unique for the whole chromosome, or that can change for every gene. The genes of the offspring are bounded by the genes of the parents, no value can be out of their range.
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = \beta g_{p1,n} + (1-\beta)g_{p2,n} \\ 
	g_{o2,n} = (1-\beta) g_{p1,n} + \beta g_{p2,n} \\
	\end{array} \right.
	\label{equation_mating_blending_method}
	\end{equation}
	
	\item Linear crossover \citep{Wright1991a}: in order to allow the genes to take values outside the interval defined by the parents, a method of extrapolation is necessary. Linear crossover introduces such an approach, and produces three children from two parents, following equation \ref{equation_mating_linear_crossover}. Less couples are required in order to fill up the population.
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = 0.5 g_{p1,n} + 0.5 g_{p2,n} \\ 
	g_{o2,n} = 1.5 g_{p1,n} - 0.5 g_{p2,n} \\ 
	g_{e3,n} = - 0.5 g_{p1,n} + 1.5 g_{p2,n} \\ 
	\end{array} \right.
	\label{equation_mating_linear_crossover}
	\end{equation}
	
	\item Heuristic crossover \citep{Michalewicz1996}: it is a variation of the latter methods that relies on the following equation:
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = \beta (g_{p1,n} - g_{p2,n}) + g_{p1,n} \\
	g_{o2,n} = \beta (g_{p2,n} - g_{p1,n}) + g_{p2,n} \\
	\end{array} \right.
	\label{equation_mating_heuristic_crossover}
	\end{equation}
	
	\item Linear interpolation: unlike previous techniques, this technique does not rely on crossover points, but on a linear interpolation on every gene of the couple (equation \ref{equation_mating_linear_interpolation}).
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	c_{o1} = c_{p1} - \beta (c_{p1} - c_{p2}) \\
	c_{o2} = c_{p2} + \beta (c_{p1} - c_{p2}) \\
	\end{array} \right.
	\label{equation_mating_linear_interpolation}
	\end{equation}
	where $c_{o1}$ and $c_{o2}$ are the full chromosomes of the offspring, and $c_{p1}$ an $c_{p2}$ are the ones of the parents. As before, $\beta$ is a random value between 0 and 1, and is here the same for every gene.
	
	\item Free interpolation: this technique performs interpolation on each gene, like the previous one; but in this case, the weighting factor changes for each gene:
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	c_{o1} = c_{p1} - [\beta_{1} (g_{p1,1} - g_{p2,1}), \beta_{2} (g_{p1,2}\\
	~~~~~~~~~~~~ - g_{p2,2}), ..., \beta_{Ng} (g_{p1,N_{g}} - g_{p2,N_{g}})] \\
	c_{o2} = c_{p2} + [\beta_{1} (g_{p1,1} - g_{p2,1}), \beta_{2} (g_{p1,2}\\
	~~~~~~~~~~~~ - g_{p2,2}), ..., \beta_{Ng} (g_{p1,N_{g}} - g_{p2,N_{g}})] \\
	\end{array} \right.
	\label{equation_mating_free_interpolation}
	\end{equation}
	where $N_{g}$ is the number of genes, and $\beta$ is here independent between the genes.
	
\end{itemize}

Many other methods or variations exist, combining the advantages of different approaches. The performance of the variants being related to the problem to be addressed, one cannot identify a priori the best technique for any application.


\subsubsection{Mutation}
\label{sec:gas:mutation}

The combination of strong genes by the operator of chromosomes crossover is theoretically the most important operating mechanism in the conventional GAs \citep{Holland1992b,Back1993b}. However, many studies identify the mutation process as main operator, and crossovers as secondary \citep[see][]{Back1992a, Back1996a, Back1996b, Smith1997a, Deb1999, Costa2005a, Costa2007a}.

The mutation operator is a direct modification of genes. In real coding, it is done by changing the gene values. Mutations add diversity to the population and prevent a freeze of the evolution, or a genetic drift to a local optimum. Thus, it makes the convergence to the global optimum theoretically possible \citep{Beasley1993a}, as they allow exploring beyond the current region of the parameter space by bringing new characteristics that were not present in the original population \citep{Haupt2004}. 

The evaluated and developed mutation operators are the following:

\begin{itemize}
	\item Uniform mutation: The mutation rate is constant and equal for every gene of each individual; they all have the same probability to mutate. When a gene is selected for mutation, a new random value is assigned, according to a uniform law.
	
	\item Variable uniform mutation \citep{Fogarty1989}: a variable mutation rate over the generations was first suggested by \citet{Holland1992b} and evaluated by \citet{Fogarty1989}. In most applications, the mutation rate decreases with the generations, in a deterministic and global (for all individuals) manner \citep{Back1992b}. Its optimum configuration depends on the size of the chromosomes, of the properties of the objective function, and of the population size \citep{Back1992b}. We implemented this operator according to equation \ref{equation_mutation_uniformvariable}.
	\begin{equation}
	p_{n,G} = p_{G_{0}}+\left( \dfrac{p_{G_{0}}-p_{G_{m,p}}}{G_{m,p}} \right) min\left\lbrace G,G_{m,p}\right\rbrace 
	\label{equation_mutation_uniformvariable}
	\end{equation}
	where $p_{n,G}$ is the mutation rate (probability) of the gene $n$ for generation number $G$, $G_{m,p}$ is the maximum number of generations during which the mutation rate varies. $p_{G_{0}}$ is the initial mutation probability, and $p_{G_{m,p}}$ is the final one. $p_{G_{0}}$, $p_{G_{m,p}}$ and $G_{m,p}$ are the three controlling parameters of the operator. The evolution of the mutation rate is linear.
	
	\item Constant normal mutation: many applications use normal distributions to generate new values. The gene $g$ that mutate becomes:
	\begin{equation}
	g' = N(g,\sigma^{2})
	\label{equation_mutating_normal_distribution}
	\end{equation}
	where $\sigma$ is the standard deviation of the distribution. The disadvantage of this technique is that an accurate value of $\sigma$ must be chosen \citep{Haupt2004}, which is impossible to know beforehand.
	
	\item Variable normal mutation: with the same logic as the variable uniform mutation, we tested a mutation operator using a normal distribution with a variable mutation rate and standard deviation. The mutation rate is calculated with equation \ref{equation_mutation_uniformvariable}. On the same principle, we decrease linearly the standard deviation over the generations:
	\begin{equation}
	\sigma_{n,G} = \sigma_{G_{0}}+\left( \dfrac{\sigma_{G_{0}}-\sigma_{G_{m,\sigma}}}{G_{m,\sigma}} \right) min\left\lbrace G,G_{m,\sigma}\right\rbrace 
	\label{equation_mutation_normalvariable}
	\end{equation}
	where $\sigma_{n,G}$ is the standard deviation of gene $n$ and generation number $G$, $\sigma_{G_{0}}$ is the initial standard deviation, $\sigma_{G_{m,\sigma}}$ is the final standard deviation, $G_{m,\sigma}$ is the maximum number of generations during which the standard deviation varies. $p_{G_{0}}$, $p_{G_{m,p}}$, $G_{m,p}$, $\sigma_{G_{0}}$, $\sigma_{G_{m,\sigma}}$ and $G_{m,\sigma}$ are the six parameters of the method.
	
	\item Non-uniform mutation \citep{Michalewicz1996}: two random numbers are picked based on a uniform law: $r_{1}$, which determines the direction of the change, and $r_{2}$, which determines its magnitude. The new value of the gene is given by the following equation:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} \left(1 - \dfrac{G}{G_{m}} \right)^{2} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} \left(1 - \dfrac{G}{G_{m}} \right)^{2} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_nonuniform_original}
	\end{equation}
	where $a_{n}$ is the is the lower bound of the $n$-th gene, $b_{n}$ its upper bound, $G$ the present generation, and $G_{m}$ the maximum number of generations.
	
	We adapted this operator for our application, which is not based on a predefined number of generations:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} \varphi^{2} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} \varphi^{2} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_nonuniform}
	\end{equation}
	
	with 
	
	\begin{equation}
	\varphi = 1 - \min \left\lbrace \dfrac{G}{G_{m,r}}, 1 \right\rbrace \left(1-\omega\right)
	\end{equation}
	
	where $G_{m,r}$ is the maximum number of generations during which the magnitude of the research varies, and $\omega$ is a threshold chosen by the user to maintain a minimum search radius when $G>G_{m,r}$. During the first generations, the exploration extent covers the entire parameter space. However, this area is reduced over generations, allowing exploitation of local solutions.
	
	\item Individual adaptive mutation rate \citep{Back1992a}: based on the ideas of Evolution Strategies \citep[see][]{Rechenberg1973, Schwefel1981}, \citet{Back1992a} introduced a concept of self-adaptive GAs. The idea is to distribute control parameters within individuals themselves, which partially decentralize control of the evolution. It allows reducing the parametrization of GAs and introducing a notion of self-management. The first approach is the introduction of a mutation rate per individual, that mutates itself under its own probability \citep{Back1992a}. Then, the eventual new rate is used to mutate the genes of the individual. Thus, as this rate decreases, it will have less probability of being itself mutated. This approach is close to the natural adaptation phenomena: a population less suited to its environment is changing faster than better adapted populations. Mutations are performed according to a constant uniform distribution. The initial mutation rates are randomly chosen \citep{Back1992a} and the method has no parameter. Other approaches exist to introduce a self-adaptation \citep[see][]{Smith1997a, Deb1999, Deb2001a}.
	
	\item Individual adaptive search radius (new): based on the ideas of the non-uniform mutation, we introduce a search radius in the approach of individual adaptive mutation rates. This search radius $r_{a}$, bounded between 0 and 1, is also adaptive and behaves similarly to the adaptive mutation rates. In order to separate its evolution from the one of the mutation rate, its own value is considered initially as a self-mutation rate to eventually mutate before being used as a normalized search radius. The value of a mutated gene is given by the following equation, which is a simplification of the non-uniform mutation:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} r_{a} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} r_{a} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_rayon_adaptatif}
	\end{equation}
	where $r_{1}$ and $r_{2}$ are randomly selected, in the same way as for the non-uniform mutation. No external parameter is therefore necessary.
	
	\item Chromosome of adaptive mutation rate \citep[or \textit{n adaptative mutation rate},][]{Back1992a}: analogously to the individual adaptive mutation rate, this approach leaves the control of the evolution rate to the individuals themselves. The difference here is that each gene has a specific mutation rate. The main advantage is that the tuning of the mutation can be much more precise \citep{Smith1997a}. We therefore consider a second chromosome containing the mutation rate for each gene of the first chromosome. The operations of mutation and self-mutation are similar to the case of the individual adaptive mutation rate, but in a distributed way, within the chromosome. Another difference is that we apply the same crossover operations as those applied to the first chromosome, and this for the same crossing points. Thus, during an exchange of genes, children also inherit the mutation rates specific for each of these genes.
	
	\item Chromosome of adaptive search radius (new): we introduced this operator that combines the operations of the chromosome of adaptive mutation rate to our adaptive search radius approach. Similarly, an individual has 3 chromosomes: the first containing the values to be optimized, the second the distributed mutation rate, and the last one, the distributed search radius. Again, no external parameters are required.
	
	\item Multi-scale mutation (new): finally, we developed another approach, that is also based on the search radius concept. However, the latter is not decreasing with time. Methods based on a reduction of the mutation rate or radius simulate a transition from the exploration phase to the exploitation one. We wanted to test an approach that combines both exploration and exploitation during the whole optimization. Thus, we considered the search radius $r_{a}$ of equation \ref{equation_mutation_rayon_adaptatif} as a random value for each individual, but restricted to 4 equiprobable values: 1, 0.5, 0.1, 0.02, which range from full exploration to fine exploitation. The only external parameter is the mutation rate which is fixed.
	
	
\end{itemize}

When the gene to mutate is represented by a list of distinct values (eg meteorological variable or analogy criterion), the random choice of a new value is always based on a uniform distribution, without notion of distance in the parameters space.


\subsubsection{Elitism}

We used a process of elitism on the natural selection as well as on mutations. This ensures the survival of the best individual so that we do not lose a better solution. After the natural selection operator, if the best individual has not been selected, it is copied to the IG instead of an individual randomly picked. After mutation, if the best individual has mutated and if its new version has a lower performance score than the original, the latter is also reinserted in the IG instead of an individual randomly chosen.


\subsubsection{Ending the optimization}

The convergence check determines whether the solution is acceptable and if the algorithm may stop. The stopping criteria are not often well documented in GAs case studies. We chose to stop the optimization if the best individual does not change for $x$ generations. This value should not be too low to allow the algorithm to escape from a local optima. In addition, the rate of improvement decreases with the progression of the optimization. It is thus common that the best individual does not evolve over several generations when we get closer to the global solution. A value of $x=20$ generations was chosen.


\subsection{Implementation and constraints}

Some constraints need to be taken into account. For example, when a crossover or a mutation operation results in a parameter value standing out of the authorized bounds, it has to be brought back within the limits. Moreover, the parameters are of different nature: some are continuous, such as the weighting between pressure levels, some are discretized, such as the analogues number, or the spatial windows, and finally, some are independent elements in an array, such as the selection of the meteorological variable. New values resulting from the optimizer need to respect the type of data it represents.

Other constraints exist in between the parameters, such as the temporal window of the moisture index \cite[see][]{Horton2016} that has to be consistent in between the relative humidity and the precipitable water. Another example is the weighting of the different pressure levels which has to be normalized.

GAs are very computationally intensive because they require many evaluations of the objective function. These assessments are very long in our application, as they require calculating and assessing a forecast for every day of the calibration period, thus over several decades. In order to reduce the computation time, we avoid recalculating the performance score of an individual who has previously been evaluated and that has not changed. We keep the score of each individual living in the selection until it mutates.

As the assessment (calculation of the objective function) of each member of the population of a generation is completely independent, it was performed in parallel on different processors of a computer, as suggested by \citet{Alliot2005}. In order to perform optimizations for multiple time series, the use of a cluster is a necessity, which our code allows.


\section{Assessment process and results}
\label{sec:assessment}

The GAs parametrization, such as the mutation rate, population size, natural selection options, and so on, is difficult given the high number of existing variants, each developed for a specific problem \citep{Haupt2004, Costa2007a}. The parametrization depends on the objective function, implementation variants, the range of the parameters to be optimized, and performance indicators. Thus, different studies suggest very different parametrization.

\citet{DeJong1975a} and \citet{Grefenstette1986} compared different implementations and parametrizations of GAs on functions of varying complexity. They observed that a small population size improves the initial performance, while a large population improves long-term performance. They also observed that the ratio of the population to keep for the IG is around 50\% (45\% to 60\%).

Values of the mutation rate varies broadly between the studies: from 0.001 \citep{DeJong1975a} to 0.2 \citep{Haupt2004}. \citet{Back1996b} showed that mutation rates higher than the usual ranges are more optimal at the beginning of the optimization, allowing further exploration. Incremental approaches with varying mutation rates are certainly more optimal but more complex to implement \citep{Back1996a, Back1996b}.


\subsection{Comparison process}

One of our goals being to make recommendations of parametrization in order to optimize the AM, we proceeded systematically. The results are summarized hereafter \citep[see][for the details]{Horton2012a}. We used concepts from the factorial design approach \citep[see eg.][]{Costa2005a,Costa2007a,Mariano2010a}, which is sometimes used for comparative analysis of different parametrizations of GAs. It allows isolating the effect of a parameter under different combinations of the other options. We processed by stages, analysing in details and in a systematic way every variants of the implemented operators, in combination with multiple other options and parameters in order to take into account eventual co-dependencies. Our goal here is not to focus on the performance score we obtain through optimization, neither the values of the new optimized parameters \citep[covered in][]{Horton2016b}, but to explain how to use GAs to optimize AMs in an efficient way.

In order to evaluate a combination of operators/options, we processed 10 optimizations per parametrization of GAs. The performances were characterized by four indicators: (i) mean performance score: average of the final scores of the 10 optimizations, (ii) convergence: the number of optimizations that converged to a supposed global optimum, (iii) number of generations: characterization of the convergence speed, and (iv) number of evaluations of the objective function: characterization of the required calculation time.


\subsection{Success of the approach}

After a first overview of the results, GAs have quicky been proven successful at optimizing the AM. Indeed, the performance score of the sequential approach has been quickly exceeded without adding any new parameter to the method (Figure \ref{fig:gas_evolution_good}). Even GAs parametrization that we will consider further as inadequate (Figure \ref{fig:gas_evolution_bad}) did significantly better that the sequential approach.


\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{figures/gas_evolution_good.pdf}\\
	\end{center}
	\caption{Evolution of the score of the best individuals over generations for the 10 optimizations processed for a given parametrization. The green line represents the score of the sequential approach and the blue one, the supposed global optimum.}
	\label{fig:gas_evolution_good}
\end{figure}


\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{figures/gas_evolution_bad.pdf}\\
	\end{center}
	\caption{Same as Figure \ref{fig:gas_evolution_good}, but for a GAs parametrization considered as less relevant.}
	\label{fig:gas_evolution_bad}
\end{figure}



\subsection{Results of the comparison}

The results illustrate the effect of an operator when its contribution is isolated from the other operators. It means that we compare the effect of the studied operator when all the other operators are equivalent, while assessing multiple combinations. We then summarize this contribution relatively to the other options, as a percentage of gain/loss regarding the mean of all variants. For example, to evaluate the performance of the uniform crossover operator, we compare its performance to the average of all crossover operators while retaining the same population size, the same mutation operators, natural selection, and selection of couples.

From the very beginning of the assessments, the importance of the mutation operator was obvious \cite[see][for the details]{Horton2012a}, and its leading influence on the optimization performance was evident. Its role is analyzed later on.

\subsubsection{Breeding operators}

We evaluated every combination of 6 options for the couples selection (Table \ref{tab:assessed_couples_selection_operators}) and 21 for the chromosome crossover operators (Table \ref{tab:assessed_crossover_operators}), along with variants of the other operators. This results in 1,008 combinations, requiring 10,080 optimizations.

\begin{table}[htbp]
	\caption{Assessed operator for couples selection.}
	\begin{center}
		\begin{tabular}{ll}
			\hline\hline  & \textbf{Couples selection operators} \\ 
			\hline 
			A & Rank pairing \\ 
			B & Random pairing \\ 
			C & Roulette wheel weighting on rank \\ 
			D & Roulette wheel weighting on fitness \\ 
			E & Tournament selection (3 candidates) \\ 
			F & Tournament selection (4 candidates) \\ 
			\hline 
		\end{tabular}
	\end{center}
	\label{tab:assessed_couples_selection_operators}
\end{table}

\begin{table}[htbp]
	\caption{Assessed operators for chromosome crossover.}
	\begin{center}
		\begin{tabular}{ll}
			\hline\hline  & \textbf{Chromosome crossover operators} \\ 
			\hline 
			1 & Single-point crossover \\
			2 & Two-point crossover \\
			3 & Multiple-point crossover (3 points) \\
			4 & Multiple-point crossover (5 points) \\
			5 & Uniform crossover \\
			6 & Blending method (2 points, unshared $\beta$) \\
			7 & Blending method (4 points, unshared $\beta$) \\
			8 & Blending method (2 points, shared $\beta$) \\
			9 & Blending method (4 points, shared $\beta$) \\
			10 & Linear crossover (2 points) \\
			11 & Linear crossover (4 points) \\
			12 & Heuristic crossover (2 points, unshared $\beta$) \\
			13 & Heuristic crossover (4 points, unshared $\beta$) \\
			14 & Heuristic crossover (2 points, shared $\beta$) \\
			15 & Heuristic crossover (4 points, shared $\beta$) \\
			16 & Binary-like crossover (2 points, unshared $\beta$) \\
			17 & Binary-like crossover (4 points, unshared $\beta$) \\
			18 & Binary-like crossover (2 points, shared $\beta$) \\
			19 & Binary-like crossover (4 points, shared $\beta$) \\
			20 & Linear interpolation \\
			21 & Free interpolation \\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:assessed_crossover_operators}
\end{table}

\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=7cm,angle=0]{figures/operator_selectcoupl_score.pdf}\\
	\end{center}
	\caption{Influence of the couples selection operators (Table \ref{tab:assessed_couples_selection_operators}) on the optimization performance. The box extends from the lower to upper quartile values of the data, with a line at the median. The whiskers extend from the box to 1.5 times the interquartile range. Flier points are those past the end of the whiskers. The star represents the median. The gray box highlights the best options.}
	\label{fig:operator_selectcoupl_score}
\end{figure}


\begin{figure*}[htb]
	\begin{center}
		\noindent\includegraphics[width=16cm,angle=0]{figures/operator_crossover_score.pdf}\\
	\end{center}
	\caption{Influence of the chromosome crossover operators (Table \ref{tab:assessed_crossover_operators}) on the optimization performance. Same conventions as Figure \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_crossover_score}
\end{figure*}

The performance of the couples selection operator are relatively close (Figure \ref{fig:operator_selectcoupl_score}). Overall, we observe that the tournament selection with 3~candidates is slightly superior to others, along with the roulette wheel weighting. This last one is however a bit less effective in terms of convergence and number of evaluations (not shown). The couples selection operator has not a significant role in our application. 

Analysis of crossover operators (Figure \ref{fig:operator_crossover_score}) reveal some slightly superior options, some inappropriate, and many averages. Binary-like crossover (especially with 2 points of intersection, whether $\beta$ is shared or not) are significantly better than the others, especially in terms of convergence (not shown). Others operators can also be considered usable.
	

\subsubsection{Mutation operator}
\label{sec:assessment:mutation}

Having identified the leading role of the mutation operator, the sensitivity analysis focused on it. Each of the 10 different implementations (see section \ref{sec:gas:mutation}) was tested, with different parameters for those who require some (Table \ref{tab:assessed_mutation_operators}), bringing the number of its variations up to 109. We also did some optimizations without any mutation as a reference. Along with variants of the other operators \citep[see][for the details]{Horton2012a}, it resulted in 660 combinations (so 6,600 optimizations).

\begin{table}[htbp]
	\caption{Assessed mutation operators with the number of variants considered (combination of parameters).}
	\begin{center}
		\begin{tabular}{llc}
			\hline\hline  & \textbf{Mutation operator} & \textbf{Variants}\\ 
			\hline 
			1 & Uniform mutation & 3 \\
			2 & Variable uniform mutation & 27 \\
			3 & Constant normal mutation & 9 \\
			4 & Variable normal mutation & 36 \\
			5 & Non-uniform mutation & 27 \\
			6 & Individual adaptive mutation rate & 1 \\
			7 & Individual adaptive search radius & 1 \\
			8 & Chromosome of adaptive mutation rate & 1 \\
			9 & Chromosome of adaptive search radius & 1 \\
			10 & Mutli-scale mutation & 3 \\
			11 & No mutation & 1 \\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:assessed_mutation_operators}
\end{table}


\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=7cm,angle=0]{figures/operator_mutation_score.pdf}\\
	\end{center}
	\caption{Influence of the mutation operators (Table \ref{tab:assessed_mutation_operators}) on the optimization performance. Same conventions as Figure \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_mutation_score}
\end{figure}


Figure \ref{fig:operator_mutation_score} show the results of this analysis and illustrates the important role of the mutation on the performance of the optimizations. Those without mutation (last box on the Figure) are inferior to most mutation operators, and the scale of the influence of this operator is significantly more important than those for the other options. The details of the analysis \citep[see][]{Horton2012a} show that the other reproduction operators seem of secondary importance. This observation is in line with the work of \citet{Back1996a}, who argues for the importance of mutation over reproduction. He even suggests, in opposition with the theory of \citet{Holland1992a}, that chromosomes crossovers have mostly a corrective role of mutation operations. Various studies have also identified the importance of the mutation operator relatively to reproduction \citep[see eg.][]{Back1992a, Back1996b, Smith1997a, Deb1999, Haupt2004, Costa2005a, Costa2007a}.

The mutation operators based on a variable normal or variable uniform laws work very poorly and are difficult to configure. We then observe many operators more or less with the same performance scores and requiring a variable amount of assessments. The convergence analysis \citep[see][]{Horton2012a} allows us to highlight three best operators: non-uniform mutation, chromosome of adaptive search radius, and multi-scale mutation. Because of the importance of the mutation operator on the resulting performance, we studied its role and parametrization in more detail. Thus, we performed different optimizations using variants of these 3 operators (Table \ref{tab:assessed_mutation_operators_bis}).

\begin{table}[htbp]
	\caption{Further assessments of mutation operators.}
	\begin{center}
		\begin{tabular}{llrrr}
			\hline\hline  & \textbf{Mutation operator} & \textbf{$p_{mut}$} & \textbf{$G_{max}$} & \textbf{$\omega$}\\ 
			\hline 1 & Non-uniform mutation & 0.01 & 50 & 0.1 \\
			2 & Non-uniform mutation & 0.05 & 50 & 0.1 \\
			3 & Non-uniform mutation & 0.1 & 50 & 0.1 \\
			4 & Non-uniform mutation & 0.2 & 50 & 0.1 \\
			5 & Non-uniform mutation & 0.4 & 50 & 0.1 \\
			6 & Non-uniform mutation & 0.01 & 100 & 0.1 \\
			7 & Non-uniform mutation & 0.05 & 100 & 0.1 \\
			8 & Non-uniform mutation & 0.1 & 100 & 0.1 \\
			9 & Non-uniform mutation & 0.2 & 100 & 0.1 \\
			10 & Non-uniform mutation & 0.4 & 100 & 0.1 \\
			11 & Mutli-scale mutation &  0.01 &&\\
			12 & Mutli-scale mutation &  0.05 && \\
			13 & Mutli-scale mutation &  0.1 && \\
			14 & Mutli-scale mutation &  0.2 && \\
			15 & Mutli-scale mutation &  0.4 && \\
			16 & \multicolumn{4}{l}{Chromosome of adaptive search radius} \\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:assessed_mutation_operators_bis}
\end{table}

\begin{figure*}[htb]
	\begin{center}
		\noindent\includegraphics[width=14cm,angle=0]{figures/operator_mutation_score_atmlevel.pdf}\\
	\end{center}
	\caption{Influence of the mutation operators (Table \ref{tab:assessed_mutation_operators_bis}) on the optimization performance, leaving the optimizer choose the pressure level of the atmospheric circulation analogy (single level of analogy). The predictand is precipitation over a subcatchment in the Swiss Alps (Binn-Simplon region). The red line represents the score of the sequential calibration and the blue line, the score of the optimization without automatic selection of the pressure levels. Same conventions as Figure \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_mutation_score_atmlevel}
\end{figure*}

\begin{figure*}[htb]
	\begin{center}
		\noindent\includegraphics[width=14cm,angle=0]{figures/operator_mutation_score_rhoneamont.pdf}\\
	\end{center}
	\caption{Same as Figure \ref{fig:operator_mutation_score_atmlevel}, but for another region in the Swiss Alps (Bottom Rhone valley), with different atmospheric influences.}
	\label{fig:operator_mutation_score_rhoneamont}
\end{figure*}

\begin{figure*}[htb]
	\begin{center}
		\noindent\includegraphics[width=14cm,angle=0]{figures/operator_mutation_score_r2.pdf}\\
	\end{center}
	\caption{Same as Figure \ref{fig:operator_mutation_score_atmlevel}, but with a second level of analogy on moisture variables.}
	\label{fig:operator_mutation_score_r2}
\end{figure*}

\begin{figure*}[htb]
	\begin{center}
		\noindent\includegraphics[width=14cm,angle=0]{figures/operator_mutation_score_r4.pdf}\\
	\end{center}
	\caption{Same as Figure \ref{fig:operator_mutation_score_r2}, but with a preselection on air temperature rather than a fixed calendar window.}
	\label{fig:operator_mutation_score_r4}
\end{figure*}

The first analysis was the optimization of the precipitation forecasting over a subcatchment (Binn-Simplon region) in the Swiss Alps (Figure \ref{fig:operator_mutation_score_atmlevel}). The optimizer could choose the 2 pressure levels of the atmospheric circulation analogy (method with a single level of analogy). The resulting performance score \citep[CRPSS = skill score of the CRPS, see][]{Brown1974} is obviously superior to the one from the sequential calibration (red line on Figure \ref{fig:operator_mutation_score_atmlevel}). For most options, it is also slightly better than the results from the optimization without selection of the pressure levels (blue line). A clear breakthrough of the performances was not expected, as the former selection of pressure levels results from intensive work \citep{Bontron2004}. This application however demonstrates that, when correctly parametrized, GAs can automatically and successfully choose the pressure levels. However, some less relevant optimizations do not match the previous results. We could clearly observe, through different applications, that the automatic selection of the pressure level significantly increases the difficulty for GAs to converge to a unique solution, ideally the global optimum. A difficulty is that the pressure levels are considered within the optimization without continuity between values, and thus the approaches relying on distance in the parameters space, such as the search radius, cannot fully exploit the properties that make them efficient. However, even though the results show a certain variability, most of them present very good performance scores, despite different parameters of the AM.

Then, we performed the same optimization, but for another region, sensitive to other meteorological influences (Figure \ref{fig:operator_mutation_score_rhoneamont}), in order to assess eventual dependencies of the operator with the predictand. Even though we can observe differences with Figure \ref{fig:operator_mutation_score_atmlevel}, it is globally the same options that perform better.

Next, we added a second level of analogy (Figure \ref{fig:operator_mutation_score_r2}) on moisture variables \citep[see][]{Bontron2004, Horton2016}. GAs had to optimize both levels of analogy simultaneously. Once again, despite the difficulty to optimize simultaneously both levels of analogy, the results were better than the sequential calibration (red line on Figure \ref{fig:operator_mutation_score_r2}). And finally, we added a preselection on air temperature instead of the fixed calendar window, as proposed by \cite{BenDaoud2015}. The results show generally higher scores (Figure \ref{fig:operator_mutation_score_r4}), demonstrating the success of the optimizer to take advantage of this new degree of freedom, and its capacity to handle optimization of 3 analogy levels jointly. Again, the most relevant options are generally the same.

After analysis of the most relevant mutation operators, we can raise the following advice (detailed parametrization are provided in section \ref{sec:recommendations}):

\begin{itemize}
	
	\item \textit{Non-uniform mutation} \citep{Michalewicz1996}: this operator is good in terms of convergence, mainly when the number of parameters to optimize is rather low. The number of required evaluations, however, can be quite substantial. The main disadvantage of the non-uniform mutation is the number of parameters it requires, that are difficult to estimate a priori. We could observe that the $\omega$ coefficient does not influence performance. The role of $G_{m}$ is rather difficult to judge, but does not seem essential. The mutation rate was found to be important. The difficulty is that the optimal value may be case-related.
	
	\item \textit{Chromosome of adaptive search radius} (new): unlike the previous one, our new operator is very robust, as it requires no option and is auto-adapting. It is interesting to notice that our insertion of an extra chromosome representing the search radius gives better performance than other self-adaptive operators (such as, for example, the chromosome of adaptive mutation rate). If one had to choose a single option for the mutation operator, we would recommend this one, as it was proven effective and needs no parameter.
	
	\item \textit{Multi-scale mutation} (new): finally, the multi-scale mutation, which also performs pretty well, requires one parameter, the mutation rate. However, it can also be difficult to estimate a correct value a priori.
	
\end{itemize}

In our application, the mutation operator has a leading effect and should be chosen with care. It may be wise to perform multiple optimizations and to consider these three operators in parallel in order to obtain results from options that are either sometimes more efficient or more robust. It is interesting to note that the three best techniques incorporate a notion of search distance. It is likely that this notion is the key to these algorithms, for our application, and allows them to initially explore the parameter domain, and then to converge. The search radius in fact directly represents the notion of transition between exploration and exploitation, in our opinion more than a possible evolution of mutation rates.


\subsubsection{Other options}

The analysis of the natural selection operator (Figure \ref{fig:operator_selectnat_score}) reveals a slight preference for the ratio-elitism compared to the tournament selection, but not so significant. This operator, or at least the two assessed versions, do not appear to significantly influence the optimization performances.

\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=4cm,angle=0]{figures/operator_selectnat_score.pdf}\\
	\end{center}
	\caption{Influence of the natural selection operators on the optimization performance. Same conventions as Figure \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_selectnat_score}
\end{figure}

The size of the population ($N$) has an effect on the performance of the optimization (Figure \ref{fig:option_taillepop_score}). A bigger population leads to better results, but also to significantly longer optimizations. Indeed, the required number of evaluation, and thus the required time, is approximately proportional to the population size. The optimal size seems to depend on the complexity of the AM to optimize: a more complex AM (ie. with more degrees of freedom) requires a bigger population size. A rule of thumb based on a limited number of case studies (not shown here) is provided hereafter:

\begin{itemize}
	\item $N\approx100$ for a very simple implementations of the AM (1 level of analogy with 2 pressure levels),
	\item $N\approx200$ for a bit more complex AM (1 level of analogy with 4 pressure levels, or 2 level of analogy with less pressure levels),
	\item $N\approx500$ for significantly more complex AMs (2-3 levels of analogy with 4 pressure levels for the atmospheric circulation, and 2 to 4 levels for the moisture analogy).
\end{itemize}

\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=4cm,angle=0]{figures/option_taillepop_score.pdf}\\
	\end{center}
	\caption{Influence of the population size on the optimization performance. Same conventions as Figure \ref{fig:operator_selectcoupl_score}.}
	\label{fig:option_taillepop_score}
\end{figure}

We also assessed the influence of the size of the IG (ratio of the total population) selected for mating (Figure \ref{fig:option_popratio_score}). It does not appear that this parameter is critical to the quality of the optimizations, provided it is not too big. A value of 50~\% seems a wise choice.

\begin{figure}[htb]
	\begin{center}
		\noindent\includegraphics[width=4cm,angle=0]{figures/option_popratio_score.pdf}\\
	\end{center}
	\caption{Influence of the intermediate population (IG) ratio on the optimization performance. Same conventions as Figure \ref{fig:operator_selectcoupl_score}.}
	\label{fig:option_popratio_score}
\end{figure}


\section{Recommended parametrization of GAs}
\label{sec:recommendations}

We evaluated the optimization by GAs on AMs of varying complexity, with a large number of combinations of operators to be able to make recommendations for optimizing the AM. Our conclusions are:

\begin{itemize}
	\item The population size should be in accordance with the complexity of the AM to optimize: from 100 for the simple ones, up to 500 for the most complex AMs.
	
	\item The value of the ratio for the IG is not so important, and value of 50\% seems quite appropriate.
	
	\item Ratio-elitism is slightly better than tournaments for the natural selection operator, but it is not decisive.
	
	\item The performance of the operators for the couples selection perform relatively similarly. The roulette wheel weighting and the tournament selection are more efficient in terms of convergence and required number of evaluations.
	
	\item Most crossover operators have relatively similar performance. Binary-like crossover with two points of intersection are better than others, especially for convergence.
	
	\item Mutation has a clearly dominant influence. Three mutation operators stand out, two of which are new: the non-uniform mutation, the multi-scale mutation, and the chromosome of adaptive search radius. The latter is the most robust as it has no controlling parameter.
	
\end{itemize}

The optimization does not systematically converge to the global optimum  (but still often nearby), which is why we recommend doing several optimizations in parallel in order to compare the results, analyze the convergence, and keep the best. It may be wise to consider the three mutation operators in parallel. It would then combine algorithms that are sometimes faster and other that are more robust. In order to be confident in the optimized AMs, we propose using a set of the following mutation operators:

\begin{itemize}
	\setlength\itemsep{-4px}
	\item 1x non-uniform, $p_{mut}=0.05$, $G_{m}=50$, $\omega=0.1$
	\item 1x non-uniform, $p_{mut}=0.05$, $G_{m}=100$, $\omega=0.1$
	\item 1x non-uniform, $p_{mut}=0.1$, $G_{m}=100$, $\omega=0.1$
	\item 1x multi-scale,  $p_{mut}=0.1$
	\item 2x chromosome of adaptive search radius
\end{itemize}


\section{Conclusions}

In order to automatically optimize the AM and to get rid of the limitations of the usual sequential calibration, GAs were evaluated. Given the large number of existing operators and options, we had to evaluate systematically multiple variants to identify which operators are important, and which variant works best for the AM. We could thus identify that the mutation operator is a key element for our application, and provided new variants that were found to be efficient, such as the chromosome of adaptive search radius that is very robust (no control parameter). Recommendations were established for a relevant use of GAs for the optimization of AMs. 

We do not exclude that another global optimization methods or other operators of GAs may perform better. The relevance of such approach has now been proven as it results in parametrization of AMs that are relevant and fully automatically established. A global optimization is the only way to take into account all the dependencies between parameters and levels of analogy.

The global optimization approach allow easily adapting the AM to new regions by taking into account local meteorological influences, and has thus a great potential of use. Moreover, it allows exploring automatically datasets in order to extract the most relevant variables. It is thus possible to try assessing other predictands, such as the temperature, the limit of snowfall, the occurrence of hail, or wind, while leaving the algorithms select the best variables and the associated parameters.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\acknowledgments
Thanks to Hamid Hussain-Khan of the University of Lausanne for his help and availability, and for the intensive use of the cluster he is in charge of. Thanks to Renaud Marty for his fruitful collaboration over the years. Thanks to Dominique B\'{e}rod for his support.

Thanks to the Swiss Federal Office for Environment (FOEV), the Roads and Water courses Service, Energy and Water Power Service of the Wallis Canton and the Water, Land and Sanitation Service of the Vaud Canton who financed the MINERVE (Mod\'{e}lisation des Intemp\'{e}ries de Nature Extr\^{e}me des Rivi\`{e}res Valaisannes et de leurs Effets) project which started this research. The fruitful collaboration with the Laboratoire d’Etude des Transferts en Hydrologie et Environnement of the Grenoble Institute of Technology (G-INP) was made possible thanks to the Herbette Foundation. NCEP reanalysis data provided by the NOAA/OAR/ESRL PSD, Boulder, Colorado, USA, from their Web site at http://www.esrl.noaa.gov/psd/. Precipitation time series provided by MeteoSwiss. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIXES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Use \appendix if there is only one appendix.
%\appendix

% Use \appendix[A], \appendix}[B], if you have multiple appendixes.
%\appendix[A]

%% Appendix title is necessary! For appendix title:
%\appendixtitle{}

%%% Appendix section numbering (note, skip \section and begin with \subsection)
% \subsection{First primary heading}

% \subsubsection{First secondary heading}

% \paragraph{First tertiary heading}

%% Important!
%\appendcaption{<appendix letter and number>}{<caption>} 
%must be used for figures and tables in appendixes, e.g.,
%
%\begin{figure}
%\noindent\includegraphics[width=19pc,angle=0]{figure01.pdf}\\
%\appendcaption{A1}{Caption here.}
%\end{figure}
%
% All appendix figures/tables should be placed in order AFTER the main figures/tables, i.e., tables, appendix tables, figures, appendix figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make your BibTeX bibliography by using these commands:
\bibliographystyle{ametsoc2014}
% \bibliography{references}
\bibliography{../_refs/_articles-gas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Enter tables at the end of the document, before figures.
%%
%
%\begin{table}[t]
%\caption{This is a sample table caption and table layout.  Enter as many tables as
%  necessary at the end of your manuscript. Table from Lorenz (1963).}\label{t1}
%\begin{center}
%\begin{tabular}{ccccrrcrc}
%\hline\hline
%$N$ & $X$ & $Y$ & $Z$\\
%\hline
% 0000 & 0000 & 0010 & 0000 \\
% 0005 & 0004 & 0012 & 0000 \\
% 0010 & 0009 & 0020 & 0000 \\
% 0015 & 0016 & 0036 & 0002 \\
% 0020 & 0030 & 0066 & 0007 \\
% 0025 & 0054 & 0115 & 0024 \\
%\hline
%\end{tabular}
%\end{center}
%\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Enter figures at the end of the document, after tables.
%%
%
%\begin{figure}[t]
%  \noindent\includegraphics[width=19pc,angle=0]{figure01.pdf}\\
%  \caption{Enter the caption for your figure here.  Repeat as
%  necessary for each of your figures. Figure from \protect\cite{Knutti2008}.}\label{f1}
%\end{figure}





\end{document}