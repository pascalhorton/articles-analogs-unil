%% Version 4.3.2, 25 August 2014
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template.tex --  LaTeX-based template for submissions to the 
% American Meteorological Society
%
% Template developed by Amy Hendrickson, 2013, TeXnology Inc., 
% amyh@texnology.com, http://www.texnology.com
% following earlier work by Brian Papa, American Meteorological Society
%
% Email questions to latex@ametsoc.org.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Start with one of the following:
%% DOUBLE-SPACED VERSION FOR SUBMISSION TO THE AMS
\documentclass{ametsoc}


%% TWO-COLUMN JOURNAL PAGE LAYOUT---FOR AUTHOR USE ONLY
%\documentclass[twocol]{ametsoc}


\usepackage{gensymb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% To be entered only if twocol option is used

\journal{jcli}

%  Please choose a journal abbreviation to use above from the following list:
% 
%   jamc     (Journal of Applied Meteorology and Climatology)
%   jtech     (Journal of Atmospheric and Oceanic Technology)
%   jhm      (Journal of Hydrometeorology)
%   jpo     (Journal of Physical Oceanography)
%   jas      (Journal of Atmospheric Sciences)	
%   jcli      (Journal of Climate)
%   mwr      (Monthly Weather Review)
%   wcas      (Weather, Climate, and Society)
%   waf       (Weather and Forecasting)
%   bams (Bulletin of the American Meteorological Society)
%   ei    (Earth Interactions)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Citations should be of the form ``author year''  not ``author, year''
\bibpunct{(}{)}{;}{a}{}{,}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% To be entered by author:

%% May use \\ to break lines in title:

\title{Global Optimization of an Analog Method by Means of Genetic Algorithms}

%%% Enter authors' names, as you see in this example:
%%% Use \correspondingauthor{} and \thanks{Current Affiliation:...}
%%% immediately following the appropriate author.
%%%
%%% Note that the \correspondingauthor{} command is NECESSARY.
%%% The \thanks{} commands are OPTIONAL.

    %\authors{Author One\correspondingauthor{Author One, 
    % American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}
% and Author Two\thanks{Current affiliation: American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}}

\authors{Pascal Horton\correspondingauthor{Pascal Horton, University of Bern, Hallerstrasse 12, 3012 Bern, Switzerland.}\thanks{Current affiliation: Institute of Geography \& Oeschger Centre for Climate Change Research, University of Bern, Switzerland}}

%% Follow this form:
    % \affiliation{American Meteorological Society, 
    % Boston, Massachusetts.}

\affiliation{University of Lausanne, Lausanne, Switzerland}

%% Follow this form:
    %\email{latex@ametsoc.org}

\email{pascal.horton@alumnil.unil.ch}

%% If appropriate, add additional authors, different affiliations:
    %\extraauthor{Extra Author}
    %\extraaffil{Affiliation, City, State/Province, Country}

\extraauthor{Michel Jaboyedoff}
\extraaffil{University of Lausanne, Lausanne, Switzerland}

%% May repeat for a additional authors/affiliations:

%\extraauthor{}
%\extraaffil{}

\extraauthor{Charles Obled}
\extraaffil{Universit\'{e} de Grenoble-Alpes, LTHE, Grenoble, France}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ABSTRACT
%
% Enter your abstract here
% Abstracts should not exceed 250 words in length!
%
% For BAMS authors only: If your article requires a Capsule Summary, please place the capsule text at the end of your abstract
% and identify it as the capsule. Example: This is the end of the abstract. (Capsule Summary) This is the capsule summary. 

\abstract{Analog methods are based on a statistical relationship between synoptic atmospheric variables (predictors) and local weather (predictand), which we aim at predicting. This relationship is expressed through many parameters that are usually calibrated by means of a semi-automatic sequential procedure. This calibration approach has strong limitations: it is made of successive steps and thus cannot handle parameters dependencies, and it cannot automatically optimize some parameters, such as the selection of the pressure levels and the temporal windows (hour of observation of the predictor) on which the predictors are compared.
In order to surpass these limitations, a global optimization technique was assessed, namely Genetic Algorithms, which can optimize jointly all parameters of the method and get closer to a global optimum by taking into account the parameters dependencies. Moreover, it can choose objectively parameters that were previously manually assessed, and can take into account new degrees of freedom that were unthinkable before.
These kind of optimization techniques need however to be tailored to the problem to solve. Multiple combinations of algorithms had to be assessed, and even new operators were developed, such as the \textit{chromosome of adaptive search radius} which was found to be very robust, in order to make recommendations for the use of Genetic Algorithms to optimize sevral variants of analog methods. These recommendations are the main outcome of this work. It opens new perspective for the improvement of  analog methods, and their application to new regions or to new predictands.
}

\begin{document}

%% Necessary!
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MAIN BODY OF PAPER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%% In all cases, if there is only one entry of this type within
%% the higher level heading, use the star form: 
%%
% \section{Section title}
% \subsection*{subsection}
% text...
% \section{Section title}

%vs

% \section{Section title}
% \subsection{subsection one}
% text...
% \subsection{subsection two}
% \section{Section title}

%%%
% \section{First primary heading}

% \subsection{First secondary heading}

% \subsubsection{First tertiary heading}

% \paragraph{First quaternary heading}


\section{Introduction}
\label{section_intro}

Analog methods (AMs) relies on the hypothesis that similar situations in terms of atmospheric circulation are likely to lead to similar local weather \citep{Lorenz1956, Lorenz1969, Duband1970, Bontron2005}. The principle consists in sampling a certain number of past situations based on different atmospheric variables (predictors) in order to build a probabilistic prediction for a local weather variable of interest (predictand). A common usage of AMs is for precipitation forecasting or downscaling \citep[eg.][]{Guilbaud1997, Bontron2005, Hamill2006, Bliefernicht2010, Marty2012, Horton2012, Radanovics2013, Chardon2014, Dayon2015, Hamill2015b, BenDaoud2016}, but AMs, or equivalent, were also used to predict temperature \citep{Radinovic1975, Woodcock1980, Kruizinga1983, DelleMonache2013, Caillouet2016}, wind \citep{Gordon1987, DelleMonache2013, DelleMonache2011, Vanvyve2015, Alessandrini2015, Junk2015, Junk2015c}, solar power \citep{Alessandrini2015a, Bessa2015}, snow avalanches \citep{Obled1980, Bolognesi1993}, insolation \citep{Bois1981}, and the trajectory of tropical cyclones \citep{Keenan1981, Sievers2000, Fraedrich2003}. Applications for monthly time series in many countries also exist, including Canada \citep{Shabbar1986},  Hungary \citep{Toth1989}, the Netherlands \citep{Nap1981}, and England \citep{Murray1974}, as well as seasonal predictions: \citet{Barnett1978}, \citet{Bergen1982} and \citet{Livezey1988}.

Even though the method is rather simple compared to numerical weather prediction (NWP) models, it contains a certain number of parameters one needs to determine, such as the choice of the predictor variable, its pressure level and temporal window (hour of observation of the predictor) to consider, the spatial domain to use for the comparison, as well as the analogy criteria itself, and finally the number of analog situations to keep at each subsampling level. 

AMs need to be adapted to every new region considered, because the leading meteorological influences may be location specific. Even the selection of the pressure levels and the temporal windows should be reconsidered, if not the predictor variable itself. 

A common approach to optimize the method is by means of a semi-automatic sequential calibration procedure, developed by \citet{Bontron2004}, also described in \citet{BenDaoud2016}, and extended by \citet{Radanovics2013}. It determines some parameters of the method, for each consecutive analogy level (eg. the atmospheric circulation or moisture index), sequentially. It starts by a manual selection of the meteorologic variable (e.g. geopotential height, relative humidity, etc.), the pressure level (e.g. 500 hPa), the temporal window, and the initial analog number. Then, the spatial window on which the predictors are considered is optimized by an iterative growth of the domain, and the number of analogues is reassessed. A successive level of analogy can then be added and its spatial window optimized. Parameters of the preceding levels of analogy are not reassessed, except for the number of analogue situations to keep.

The sequential calibration procedure thus allows optimizing a limited number of parameters, but the selection of predictor variables, pressure levels and temporal windows had still to be made manually before optimizing the spatial windows and the number of analogs. Testing multiple combinations of these is very combinatorial and becomes quickly cumbersome, especially when considering multiple predictors within the same level of analogy. Thus, optimizing the method with the sequential technique is laborious, as many combinations of predictors (variables, pressure levels, temporal windows) have to be assessed. Moreover, proceeding to the optimization sequentially ignores potential dependencies between the parameters of the method, may they be within a single level of analogy or between them, which could lead to another configuration if the parameters were calibrated together. Simultaneous calibration of all parameters has never been undertaken so far. Thus, due to the sequential approach, the risk of ending in a local optimum is high and can not be avoided. Indeed, during an AM calibration it has been found that the resulting parameters may vary with initial choices (such as the number of analogs).

When creating the sequential calibration procedure, \citet{Bontron2004} was aware of the problem of dependencies between parameters and wrote: ''\textit{We perceive here the combinatorial aspect of our problem: variables and spatial windows are not independent. We will present our results by first searching the best variable [note: e.g. selection of the pressure level and the temporal window for the geopotential height] on a chosen spatial window, and next, the best window for the chosen variable. However, even by repeating the process, are we sure to obtain the optimal combination?}''. And later in his work: ''\textit{Our approach, which is again to vary the parameters one by one -- the others being fixed in a more or less arbitrary manner -- may therefore not exactly lead us to the optimal solution}''. \citet{Bliefernicht2010} has also faced the combinatorial issue of the parameters of an AM and concludes that one needs to be an expert to have a sense of their respective influence, sensitivity and nonlinear interactions. \citet{BenDaoud2010}, when calibrating an AM, also stated that ''\textit{the combinatory aspect related to the calibration was found to be too high for all the parameters to be calibrated simultaneously}''.

Another optimization strategy has been proposed by \citet{Junk2015}, that allows for an automatic optimization of weights applied to the different predictors when processing the analogy criteria (distance function). Their strategy consists in a brute-force assessment of all possible combinations. This approach is possible in their analog method, as predictors are considered at a unique point (interpolated to the location of interest), at fixed hours, and at preselected pressure levels, leaving only the weights to optimize. In the presently used analogue method (described in section \ref{sec:am}), the number of parameters to optimize makes it impossible to proceed to a brute-force strategy.

In order to overcome these limitations, two optimization techniques were assessed. First, \citet{Horton2012a} assessed the ability of the \citet{Nelder1965a} method based on a simplex algorithm. This technique did not provide satisfying results and failed at converging toward a unique solution. The parameter space of AMs can be very complex and is inappropriate for a linear optimization technique. The conclusion was that global optimization techniques were necessary in order to calibrate most AM variants, as it is the only way to optimize all parameters of all analogy levels simultaneously. In addition, it can overcome the systematic manual assessments of all pressure levels and temporal windows. Finally, it can open new perspectives by allowing the addition of new degrees of freedom, such as a weighting of the criteria values between the pressure levels, and the consideration of differentiated spatial windows between the pressure levels. The relevance of Genetic Algorithms (GAs) is presented here, which does not exclude that other global optimization techniques could eventually be successful. If using GAs to optimize AMs is computationally intensive, once an AM is calibrated, its use in real-time operations it very fast and lightweight.

This article is not about discussing the details of the results of an optimization with GAs, but describing how GAs are to be used in order to successfully optimize some AM variants. Indeed, GAs variants are numerous and always need to be tailored to the problem addressed. This requires intensive and systematic comparisons of operators and options in order to identify the key factors leading the optimization and the respective sensitivity of the options. Such analyses are presented here by application to the upper Rh\^{o}ne catchment in Switzerland, and will result in recommendations for the use of GAs when applied to several AM implementations. The demonstration of the benefit brought by such an approach on a specific case study is the topic of another coming paper. 

We will begin by presenting the basics of the considered AM variants (section \ref{sec:am}) and the assessed GAs options (section \ref{sec:gas}). The comparative analyses of these options are presented in section \ref{sec:assessment}, which results in recommendations formulated in section \ref{sec:recommendations} and conclusions in section \ref{sec:conclusions}.


\section{The considered analog method}
\label{sec:am}

% TODO: Add description of the method

AMs consist in searching a certain number of past situations in a meteorological archive that are the most similar, according to an analogy criteria, and to extract the observed values of the local weather variable of interest from another archive (typically local or basin averaged time series) in order to build the conditional empirical distribution considered as the probabilistic prediction for the target day (the day one wants to predict).

The present work takes place in the Perfect Prognosis framework \citep{Klein1963}, like the majority of AMs applications for precipitation prediction, and thus the meteorological archive from which the predictors are extracted is a reanalysis dataset. Oppositely, most applications for wind forecasting are based on a Model Output Statistics \citep[MOS, see][]{Glahn1972} context \citep[e.g.][]{DelleMonache2013, DelleMonache2011, Alessandrini2015, Junk2015, Junk2015c}.





Predictors can be varied: for example the geopotential height at different pressure levels on different temporal windows (time of observation). The method is usually made of several levels of analogy, leading to successive subsampling on predictors of different nature (eg. atmospheric circulation, moisture variables, vertical motion, and air temperature).

Two variants are often used for precipitation downscaling: one that relies on an analogy of the atmospheric circulation, and another that adds a second level of analogy on moisture variables \citep{Obled2002, Bontron2005, Marty2012}.

The method based on the analogy of the synoptic circulation consists in the following steps: the similarity of the atmospheric circulation of a target date with every day of the archive is assessed by processing the S1 criteria \citep{Teweles1954, Drosdowsky2003}, which is a comparison of gradients, on 2 different the geopotential heights (e.g. 500 and 1000~hPa) and over a certain spatial window:

\begin{equation}
\label{eq:S1}
S1=100 \frac {\displaystyle \sum_{i} \vert \Delta\hat{z}_{i} - \Delta z_{i} \vert}
{\displaystyle \sum_{i} max\left\lbrace \vert \Delta\hat{z}_{i} \vert , \vert \Delta z_{i} \vert \right\rbrace }
\end{equation}
where $\Delta \hat{z}_{i}$ is the forecast geopotential height difference between the \textit{i}th pair of adjacent points from the grid of the target situation, and $\Delta z_{i}$ is the corresponding observed geopotential height difference in the candidate situation. The differences are processed separately in both directions. The smaller the S1 values, the more similar the pressure fields.



To cope with seasonal effects, candidate dates are extracted within a period of 4 months centered around the target date, for every year of the archive. The $n_{1}$ (parameter to calibrate) dates with the lowest values of S1 are considered as analogs to the target day. Then, the daily observed precipitation amount of the $n_{1}$ resulting dates provide the empirical conditional distribution considered as the probabilistic prediction for the target day.

The other current variant adds a second level of analogy on moisture variables, more specifically on a moisture index made of the product of the precipitable water with the relative humidity at a certain pressure level \citep[e.g. 850~hPa, see][]{Bontron2004}. When adding a second level of analogy, $n_{2}$ dates are subsampled in the $n_{1}$ analogs on the atmospheric circulation, to end up with a smaller number of analog situations.

The CRPS \citep[Continuous Ranked Probability Score,][]{Brown1974, Matheson1976, Hersbach2000} has been often used to assess an AM performance \citep[see, e.g.,][]{Bontron2004, Bontron2005, BenDaoud2008, Horton2012, Marty2012, Radanovics2013, Chardon2014, Junk2015, BenDaoud2016, Caillouet2016}. It allows evaluating the predicted cumulative distribution functions $F(y)$, for example of the precipitation values $y$ from analog situations, compared to the observed value $y^{0}$. The better the prediction, the smaller the score. The mean CRPS of a prediction series of length $l$ can be written:

\begin{equation}
\label{eq:CRPS}
CRPS = \frac{1}{l} \sum_{i=1}^{l} \left(  \int_{-\infty}^{+\infty} \left[ F_{i}(y)-H_{i}(y-y_{i}^{0})\right]^{2} dy \right) 
\end{equation}
where $H(y-y_{i}^{0})$ is the Heaviside function that is null when $y-y_{i}^{0}<0$, and has the value 1 otherwise. The mean CRPS is averaged on the calibration, respectively the validation periods, on all days.

In order to compare the value of the score in regard to a reference, one often considers its skill score expression, and use the climatological distribution of daily precipitation as the reference. The CRPSS (\textit{Continuous Ranked Probability Skill Score}) is thus defined as following:

\begin{equation}
\label{eq:CRPSS}
CRPSS = \frac{CRPS-CRPS_{r}}{CRPS_{p}-CRPS_{r}} = 1-\frac{CRPS}{CRPS_{r}}
\end{equation}
where $CRPS_{r}$ is the CRPS value for the reference and $CRPS_{p}$ would be the one for a perfect prediction (which implies $CRPS_{p}~=~0$). A better prediction is characterized by an increase in CRPSS.


\section{Assessed Genetic Algorithms variants}
\label{sec:gas}

Genetic Algorithms \citep[GAs,][]{Holland1992b, Goldberg1989} are part of the family of Evolutionary Algorithms \citep{Back1993b, Schwefel1993}, inspired by some mechanisms of biological evolution, such as reproduction, genetic mutations, chromosomal crossovers, and natural selection. Unlike a linear or local optimization, GAs seek the global optimum on complex surfaces, theoretically without restriction, but with no guarantee to reach it \citep{Haupt2004}. The objective function to optimize (often named fitness function in this context) can be of different types \citep{Joines1996a}, but GAs must be adapted in order to perform optimally.

A key element of GAs configuration is finding the right balance between exploration and exploitation \citep{Back1992a, Smith1997a}. Exploration is characterized by a relatively high probability to assess the regions of the parameter space that have not yet been visited. This probability must be sufficiently large at the beginning of the optimization, so that the algorithm is capable of identifying the region where the global optimum is likely located. Exploitation is characterized by a local search in an area of interest, and generally makes small movements. The latter is interesting to refine the results at the end of the optimization.


\subsection{Structure and operators}

GAs optimize a population of $N$ individuals (parameter sets). Each individual contains a chromosome (set of parameters of our AM in this case). Genes are the individual parameters constituting the chromosome. They can be either categorical (e.g. choice of the meteorological variable), discrete (e.g. number of analogs to select), or continuous.

There are numerous implementation variants of GAs, often optimal for a given problem \citep{Hart1991a, Schraudolph1992a}. The differences are in the operators implementation, through significantly different algorithms, which has an important effect on the results \citep{Gaffney2010a}. Here, operators are defined as the mechanisms that modify the values of the genes to try bringing individuals (or chromosomes) closer to an optimum of the fitness function. The structure of the method (Fig. \ref{fig:structure_gas}) resulting from the work of \citet{Holland1992b} is common to most applications \citep{Back1993b}, and consists in the following steps: (1) a population of $N$ individuals (parameter sets of the AM to optimize) is randomly generated, which constitutes the initial population. The fitness (performance score or objective function) of every individual is assessed. (2) A natural selection is applied, after which only the best individuals remain, which constitutes the intermediate generation (IG), from which (3) couples are formed according to given rules. Then, (4) these couples proceed to reproduction, or chromosome crossover, to mix their genes according to the selected operator version. New children are generated in order to refill the IG back to $N$ individuals. (5) Parents and children are then subject to mutation, where some genes are randomly changed. Finally, (6) the new generation is then re-assessed, the best individual restored if degraded, and (7) according to the ending criteria, the optimization ends or starts again for another iteration.


All considered operators and their options are described in the following sections (only briefly for operators with less importance). Many other operators exist, but only the ones evaluated are presented.

\subsubsection{Genesis of the population}

A random initialization based on a uniform sampling is the most current version of the initial population generation. The size $N$ of the population is often a compromise between the computation time and the quality of the solution. $N$ must allow sufficient sampling of the solutions field \citep{Beasley1996a}, and should thus vary as a function of chromosome size (ie the number of genes or parameters to be optimized). 


\subsubsection{Natural selection}

Natural selection is performed on the basis of the objective function values. The selection allows to only keep a certain part of the population, usually half ($N/2$), which can access the IG (with $N_{IG}$ members). Several techniques exist, such as:

\begin{itemize}
	\item $N_{IG}$-elitism \citep{Michalewicz1996}: only the better half is preserved. 
	
	\item Tournament selection \citep{Michalewicz1996, Zitzler2004a}: two individuals are randomly selected and the best one is chosen, but with a certain probability.
\end{itemize}


\subsubsection{Selection of the couples}

Individuals of the IG can then reproduce, which begins with the selection of pairs (the parents). The techniques implemented in this work are the following:


\begin{itemize}
	\item Rank pairing: individuals are gathered in pairs according to their rank.
	
	\item Random pairing: individuals are randomly selected, according to a uniform law.
	
	\item Roulette wheel selection \citep{Goldberg1989}: the selection probability assigned to the individuals is proportional to their fitness, so that the most adapted individuals have a greater probability of reproduction. There are two weighting techniques : the first one according to the rank, and the second on the fitness value.
	
	\item Tournament selection: a number of individuals (2 or 3) are randomly picked and the best is kept, with a certain probability. This operation is performed twice, once for each partner.
\end{itemize}


\subsubsection{Chromosome crossover}

Once the two parents are selected for breeding, they combine their chromosomes and produce two children, bringing the number of individuals in the population back to $N$. The combination of chromosomes is carried out using a crossover operator, thereby generating two offspring having characteristics derived from both parents. It allows a mixing of genes and a potential accumulation of positive mutations. The evaluated crossover operators are the following:

\begin{itemize}
	\item Single-point crossover \citep{Goldberg1989}: the genes located after a randomly chosen point within the chromosome are exchanged in between the two parents.
	
	\item Two-point crossover: similar to the single-point crossover, but with two intersections defining the segments to be exchanged, it is considered more efficient than the previous \citep{Beasley1993a}.
	
	\item Multiple-point crossover \citep{DeJong1975a}: a generalization of the previous, with a number of crossover points up to the number of genes.
	
	\item Uniform crossover \citep{Syswerda1989}: for each gene of the chromosome, it is randomly chosen to exchange or not the values between the parents.
	
	\item Binary-like crossover \citep{Haupt2004}: in order to reproduce the behaviour present in the canonical crossover algorithm, which is applied to a binary representation of the genes \citep{Goldberg1989, Goldberg1990a, Herrera1998a}, \citet{Haupt2004} propose an operator that combines standard crossover with an interpolation approach. The genes located after a crossover point are exchanged, but the gene located at the intersection is modified according to:
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = g_{p1,n} - \beta (g_{p1,n} - g_{p2,n}) \\
	g_{o2,n} = g_{p2,n} + \beta (g_{p1,n} - g_{p2,n}) \\
	\end{array} \right.
	\label{equation_mating_as_binary}
	\end{equation}
	where $g_{o1,n}$ and $g_{o2,n}$ are the $n$-th gene of the two new offspring, and $g_{p1,n}$ and $g_{p2,n}$ are those of the two parents. $\beta$ is a random value between 0 and 1, that can be unique for the whole chromosome, or that can change for every gene.
	
	\item Blending method \citep{Radcliffe1991a}: in this approach, instead of exchanging the genes in between the chromosomes after some crossover points, these are combined by linear combination, also using a random value $\beta$.
	
	\item Linear crossover \citep{Wright1991a}: it widens the range of gene values, and produces three children from two parents.
	
	\item Heuristic crossover \citep{Michalewicz1996}: it is a variation of the blending method.
	
	\item Linear interpolation: unlike previous techniques, this one does not rely on crossover points, but on a linear interpolation on every gene of the couple, also using a random value $\beta$, which is the same for every gene.
	
	\item Free interpolation: this technique performs interpolation on each gene, like the previous one; but in this case, the weighting factor $\beta$ changes for each gene.
	
\end{itemize}


\subsubsection{Mutation}
\label{sec:gas:mutation}

The combination of strong genes by the operator of chromosomes crossover is theoretically the most important operating mechanism in the conventional GAs \citep{Holland1992b,Back1993b}. However, many studies identify the mutation process as main operator, and crossovers as secondary \citep[see][]{Back1992a, Back1996a, Back1996b, Smith1997a, Deb1999, Costa2005a, Costa2007a}.

The mutation operator is a direct modification of some gene values. It adds diversity to the population and prevents a freeze of the evolution, or a genetic drift to a local optimum. Thus, it makes the convergence to the global optimum theoretically possible \citep{Beasley1993a}, as it allows exploring beyond the current region of the parameters space by bringing new characteristics that were not present in the original population \citep{Haupt2004}. 

The evaluated and developed mutation operators are listed hereafter (equations are only provided for the operator versions that were found relevant; the user can refer to the corresponding literature for the details of the other options). They apply to genes made of continuous or discrete variables, but not categorical (eg meteorological variable or analogy criterion). In the latter case, the random choice of a new value is always based on a uniform distribution, without notion of distance in the parameters space.

\begin{itemize}
	\item Uniform mutation: the mutation rate ($p_{mut}$) is constant and equal for every gene of each individual; they all have the same probability to mutate. When a gene is selected for mutation, a new random value is assigned, according to a uniform law.
	
	\item Variable uniform mutation \citep{Fogarty1989}: a variable mutation rate over the generations was first suggested by \citet{Holland1992b} and evaluated by \citet{Fogarty1989}. In most applications, the mutation rate decreases with the generations, in a deterministic and global (for all individuals) manner \citep{Back1992b}. 
	
	\item Constant normal mutation: it uses normal distributions to generate new values of the gene, based on an estimated standard deviation.
	
	\item Variable normal mutation \citep{Horton2012a}: with the same logic as the variable uniform mutation, a mutation operator was tested using a normal distribution with a variable mutation rate and standard deviation, that decrease linearly over the generations. This operator has 6 parameters.
	
	\item Non-uniform mutation \citep{Michalewicz1996}: two random numbers are picked based on a uniform law: $r_{1}$, which determines the direction of the change, and $r_{2}$, which determines its magnitude. The new value of the gene is given by the following equation, according to a predefined number of generations:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} \varphi^{2} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} \varphi^{2} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_nonuniform_original}
	\end{equation}
	
	with 
	
	\begin{equation}
	\varphi = 1 - \dfrac{G}{G_{m}}
	\end{equation}
	
	where $a_{n}$ is the is the lower bound of the $n$-th gene, $b_{n}$ its upper bound, $G$ the present generation, and $G_{m}$ the maximum number of generations.
	
	This operator was adapted for this application, which is not based on a predefined number of generations, by changing $\varphi$ with $\varphi'$:
	
	\begin{equation}
	\varphi' = 1 - \min \left\lbrace \dfrac{G}{G_{m,r}}, 1 \right\rbrace \left(1-\omega\right)
	\end{equation}
	
	where $G_{m,r}$ is the maximum number of generations during which the magnitude of the research varies, and $\omega$ is a threshold chosen by the user to maintain a minimum search radius when $G>G_{m,r}$. During the first generations, the exploration extent covers the entire parameters space. However, this area is reduced over generations, allowing exploitation of local solutions.
	
	\item Individual adaptive mutation rate \citep{Back1992a}: based on the ideas of Evolution Strategies \citep[see][]{Rechenberg1973, Schwefel1981}, \citet{Back1992a} introduced a concept of self-adaptive GAs. The idea is to distribute control parameters within individuals themselves, which partially decentralize control of the evolution. It allows reducing the manual tuning of GAs and introduces a notion of self-management. The first approach is the introduction of a mutation rate per individual, that mutates itself under its own probability \citep{Back1992a}. Then, the eventual new rate is used to mutate the genes of the individual. Thus, as this rate decreases, it will have less probability of being itself mutated. Mutations are performed according to a constant uniform distribution. The initial mutation rates are randomly chosen \citep{Back1992a} and the method has no parameter. Other approaches exist to introduce a self-adaptation \citep[see][]{Smith1997a, Deb1999, Deb2001a}.
	
	\item Individual adaptive search radius (new): based on the ideas of the non-uniform mutation, a search radius was introduced in the approach of individual adaptive mutation rates. This search radius $r_{a}$, bounded between 0 and 1 (relatively to the parameters ranges), is also adaptive and behaves similarly to the adaptive mutation rates. In order to separate its evolution from the one of the mutation rate, its own value is also considered as a self-mutation rate to eventually mutate before being used as a normalized search radius. The value of a mutated gene is given by the following equation, which is a simplification of the non-uniform mutation:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} r_{a} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} r_{a} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_rayon_adaptatif}
	\end{equation}
	where $r_{1}$ and $r_{2}$ are randomly selected, in the same way as for the non-uniform mutation. No external parameter is therefore necessary.
	
	\item Chromosome of adaptive mutation rate \citep[or \textit{n adaptative mutation rate},][]{Back1992a}: analogously to the individual adaptive mutation rate, this approach leaves the control of the evolution rate to the individuals themselves. The difference here is that each gene has a specific mutation rate. The main advantage is that the automatic tuning of the mutation can be much more precise \citep{Smith1997a}. A second chromosome containing the mutation rate for each gene of the first chromosome was therefore considered. The operations of mutation and self-mutation are similar to the case of the individual adaptive mutation rate, but in a distributed way, within the chromosome. Moreover, the same crossover operations are applied to this new chromosome, as those applied to the first chromosome, and this for the same crossing points. Thus, during an exchange of genes, children also inherit the mutation rates specific for each of these genes.
	
	\item Chromosome of adaptive search radius (new): this operator combines the operations of the chromosome of adaptive mutation rate to the adaptive search radius approach. Similarly, an individual has 3 chromosomes: the first containing the values to be optimized, the second the distributed mutation rate, and the last one, the distributed search radius. Again, no external parameters are required.
	
	\item Multi-scale mutation (new): finally, another approach was developed that is also based on the search radius concept. However, the latter is not decreasing with time. Methods based on a reduction of the mutation rate or radius simulate a transition from the exploration phase to the exploitation one. The idea was to test an approach that combines both exploration and exploitation during the whole optimization. Thus, the search radius $r_{a}$ of Eq. \ref{equation_mutation_rayon_adaptatif} was considered as a random value for each individual, but restricted to 4 equiprobable values: 1, 0.5, 0.1, 0.02, which range from full exploration to fine exploitation. The only external parameter is the mutation rate which is fixed.
	
\end{itemize}


\subsubsection{Elitism}

A process of elitism was considered after both the natural selection and mutations. This ensures the survival of the best individual so that a better solution is never lost. After the natural selection operator, if the previously best individual has not been selected, it is copied to the IG instead of an individual randomly picked. After mutation, if the previously best individual has mutated and if its new version has a lower performance score than the original, the latter is also reinserted in the IG instead of an individual randomly chosen.


\subsubsection{Ending the optimization}

The convergence check determines whether the solution is acceptable and if the algorithm may stop. The optimization is here stopped if the best individual does not change for $x$ generations. This value should not be too low to allow the algorithm to escape from a local optima. In addition, the rate of improvement decreases with the progression of the optimization. It is thus common that the best individual does not evolve over several generations when getting closer to the global solution. A value of $x=20$ generations was chosen.


\subsection{Implementation and constraints}

GAs are very computationally intensive because they require many evaluations of the objective function. These assessments are very long in this application, as they require calculating and assessing a prediction for every day of the calibration period, thus over several decades. In order to reduce the computation time, recalculating the performance score of an individual who has previously been evaluated and that has not changed was avoided. Thus, the score of each individual living in the selection was kept until it mutates.

As the assessment (calculation of the objective function) of each member of the population of a generation is completely independent, it was performed in parallel on different processors of a computer. In order to perform optimizations for multiple time series, the use of a cluster is a necessity, which our code allows.

Some constraints need to be taken into account. For example, when a crossover or a mutation operation results in a parameter value standing out of the authorized bounds, it has to be brought back within the limits. Moreover, the parameters are of different nature: some are continuous, some are discrete, and finally, some are categorical, i.e. independent elements in an array, such as the selection of the meteorological variable. New values resulting from the optimizer need to respect the type of data it represents. Other constraints exist in between the parameters, e.g. the hours of observation of the relative humidity and the precipitable water predictors have to match when processing the moisture index.


\section{Assessment process and results}
\label{sec:assessment}

The choice of GAs options, such as the mutation rate, population size, natural selection options, etc, appears difficult given the high number of existing variants, each developed for a specific problem \citep{Haupt2004, Costa2007a}. Thus, different studies suggest very different configurations \citep{DeJong1975a, Grefenstette1986, Back1996a, Back1996b}.


\subsection{Comparison process}

One of our goals being to make recommendations of how to use GAs in view of optimizing AM variants, a systematic procedure was adopted. The results are summarized hereafter \citep[see][for the details]{Horton2012a}. In order to compare different configurations of GAs, the factorial design approach was applied, in a similar way as \citet{Costa2005a,Costa2007a} and \citet{Mariano2010a}. It allows isolating the effect of a parameter under different combinations of the other options. A procedure by stages was adopted, analyzing in details and in a systematic way every variant of the implemented operators, in combination with multiple other options and parameters in order to take into account eventual co-dependencies. The goal here is not to focus on the performance score obtained through optimization, neither the values of the new optimized parameters (covered in another coming paper), but to explain how to use GAs to optimize AMs in an efficient way.

In order to evaluate a combination of operators/options, 10 optimizations per configuration of GAs were processed. Such assessment is not possible on the whole archive length, and had to be performed on a reduced period. The performances were characterized by four indicators: (i) mean performance score: average of the final scores of the 10 optimizations, (ii) convergence: the number of optimizations that converged to a supposed global optimum, (iii) number of generations: characterization of the convergence speed, and (iv) number of evaluations of the objective function: characterization of the required calculation time.


\subsection{Success of the approach}

After a first overview of the results, GAs have quicky proved successful at optimizing the considered AM implementations. The performance score of the sequential approach has been quickly exceeded without adding any new parameter to the method (Fig. \ref{fig:gas_evolution_good}). Even GAs configurations that will be considered later as inadequate (Fig. \ref{fig:gas_evolution_bad}) did significantly better that the sequential approach. The amplitude of the improvement is not the main outcome here. The most important point is that GAs proved successful at optimizing AMs automatically, globally, and objectively.


\subsection{Results of the comparison}
\label{sec:assessment:results}

The results illustrate the effect of an operator when its contribution is isolated from the other operators. It means that we analyze the effect of a given operator for equivalent conditions (same settings of other operators). Multiple combinations with other operators are assessed. This contribution is then summarized as a percentage of gain/loss regarding the mean of all variants, for equivalent external conditions. For example, to evaluate the performance of the uniform crossover operator, its performance is compared to the average of all crossover operators while retaining the same population size, the same mutation operators, natural selection, and selection of couples.

From the very beginning of the assessments, the importance of the mutation operator was obvious \cite[see][for the details]{Horton2012a}, and its leading influence on the optimization performance was evident. Its role is analyzed later on in section \ref{sec:assessment}.\ref{sec:assessment:results}.\ref{sec:assessment:mutation}.

\subsubsection{Breeding operators}

Every combination of 6 options for the couples selection (Table \ref{tab:assessed_couples_selection_operators}) and 21 for the chromosome crossover operators (Table \ref{tab:assessed_crossover_operators}) were evaluated, along with variants of the other operators. This resulted in 1,008 combinations, requiring 10,080 optimizations.

The performance of the couples selection operator are relatively close (Fig. \ref{fig:operator_selectcoupl_score}). Overall, the tournament selection with 3~candidates is slightly superior to others, along with the roulette wheel weighting. This last one is however a bit less effective in terms of convergence and number of evaluations (not shown). The couples selection operator has not a significant role in this application. 

Analysis of crossover operators (Fig. \ref{fig:operator_crossover_score}) reveal some slightly superior options, some inappropriate, and many average. Binary-like crossover (especially with 2 points of intersection, whether $\beta$ is shared or not) are significantly better than the others, especially in terms of convergence (not shown).
	

\subsubsection{Mutation operator}
\label{sec:assessment:mutation}

Having identified the leading role of the mutation operator, the next sensitivity analysis focused on it. Each of the 10 different implementations (see section \ref{sec:gas:mutation}) was tested, with different parameters for those who require some (Table \ref{tab:assessed_mutation_operators}), bringing the number of variations up to 109. Some optimizations without any mutation were also performed as a reference. Along with variants of the other operators \citep[see][for the details]{Horton2012a}, this resulted in 660 combinations (so 6,600 optimizations).

Figure \ref{fig:operator_mutation_score} show the results of this analysis and illustrates the important role of the mutation on the performance of the optimizations. Those without mutation (last box on Figure \ref{fig:operator_mutation_score}) are inferior to most mutation operators, and the scale of the influence of this operator is significantly more important than those for the other options. The details of the analysis \citep[see][]{Horton2012a} show that the other reproduction operators seem of secondary importance. This observation is in line with the work of \citet{Back1996a}, who argues for the importance of mutation over reproduction, as well as other authors (see section \ref{sec:gas}.\ref{sec:gas:mutation}).

The mutation operators based on variable normal or variable uniform laws work very poorly and are difficult to configure. Many operators present more or less the same performance scores and require a variable amount of assessments. The convergence analysis \citep[see][]{Horton2012a} allows one to highlight three best operators: non-uniform mutation, chromosome of adaptive search radius, and multi-scale mutation. Thus, different optimizations were further performed using variants of these 3 operators (Table \ref{tab:assessed_mutation_operators_bis}).

The first analysis was the optimization of the precipitation downscaling over a subcatchment (Binn-Simplon region) in the Swiss Alps (Fig. \ref{fig:operator_mutation_score_atmlevel}). The optimizer could choose the 2 pressure levels of the atmospheric circulation analogy (method with a single level of analogy). The resulting CRPSS performance score (section \ref{sec:am}) is obviously superior to the one obtained by the sequential calibration (red line on Fig. \ref{fig:operator_mutation_score_atmlevel}). For most options, it is also slightly better than the results from the optimization without selection of the pressure levels (blue line). A clear breakthrough of the performances was not expected, as the former selection of pressure levels results already from intensive comparative work \citep{Bontron2004}. This application however demonstrates that, when correctly configured, GAs can automatically and successfully choose the pressure levels. However, some less relevant optimizations do not match the previous results. Through different applications, the automatic selection of the pressure level was shown to significantly increase the difficulty for GAs to converge to a unique solution, ideally the global optimum. A difficulty is that the pressure levels are considered within the optimization without continuity between values, and thus the approaches relying on distance in the parameters space, such as the search radius, cannot fully exploit the properties that make them efficient. However, even though the results show a certain variability, most of them present very good performance scores, despite different parameters of the AM considered.

Then, the same optimization was performed, but for another region, sensitive to other meteorological influences (Fig. \ref{fig:operator_mutation_score_rhoneamont}), in order to assess eventual dependencies of the operator with the predictand. Even though differences can be observed with Fig. \ref{fig:operator_mutation_score_atmlevel}, it is globally the same options that perform better.

Next, a second level of analogy was proposed (Fig. \ref{fig:operator_mutation_score_r2}) based on moisture variables \citep[see][]{Bontron2004}. GAs had to optimize both levels of analogy simultaneously. Once again, despite the difficulty to do so, the results were better than the sequential calibration (red line on Fig. \ref{fig:operator_mutation_score_r2}). And finally, a preselection on air temperature was added instead of the fixed calendar window, as proposed by \cite{BenDaoud2016}. The results show generally higher scores (Fig. \ref{fig:operator_mutation_score_r4}), demonstrating the success of the optimizer to take advantage of this new degree of freedom, and its capacity to handle optimization of 3 analogy levels jointly. Again, the most relevant options are generally the same.

After analysis of the most relevant mutation operators, the following advice can be raised (detailed options are provided in section \ref{sec:recommendations}):

\begin{itemize}
	
	\item \textit{Non-uniform mutation}: this operator is good in terms of convergence, mainly when the number of parameters to optimize is rather low. The number of required evaluations, however, can be quite substantial. The main disadvantage of the non-uniform mutation is the number of parameters it requires, that are difficult to estimate a priori. The mutation rate was found to be more important than the others. The difficulty is that its optimal value may be case-related.
	
	\item \textit{Chromosome of adaptive search radius}: unlike the previous one, the proposed operator is very robust, as it requires no option and is auto-adapting. It is interesting to notice that the insertion of an extra chromosome representing the search radius gives better performance than other self-adaptive operators (such as, for example, the chromosome of adaptive mutation rate). If one had to choose a single option for the mutation operator, we would recommend this one, as it was proven effective and needs no parameter.
	
	\item \textit{Multi-scale mutation}: finally, the multi-scale mutation, which also performs pretty well, requires one parameter, the mutation rate. However, it can also be difficult to estimate a correct value a priori.
	
\end{itemize}

In this application, the mutation operator has a leading effect and should be chosen with care. It may be wise to perform multiple optimizations and to consider these three operators in parallel in order to obtain results from options that are sometimes either more efficient or more robust. It is interesting to note that the three best techniques incorporate a notion of search distance. It is likely that this notion is the key to these algorithms, for this application, and allows them to initially explore the parameter domain, and then to converge. The search radius in fact directly represents the notion of transition between exploration and exploitation, in our opinion more than a possible evolution of mutation rates.


\subsubsection{Other options}

The analysis of the natural selection operator (Fig. \ref{fig:operator_selectnat_score}) reveals a slight preference for the ratio-elitism compared to the tournament selection, but not so significant. This operator, or at least the two assessed versions, do not appear to significantly influence the optimization performances.

The size of the population ($N$), i.e. the number of sets of AM parameters considered, has an effect on the performance of the optimization (Fig. \ref{fig:option_taillepop_score}). A bigger population leads to better results, but also to significantly longer optimizations. Indeed, the required number of evaluation, and thus the required time, is approximately proportional to the population size. The optimal size seems to depend on the complexity of the AM to optimize: a more complex AM (ie. with more degrees of freedom) requires a bigger population size. A rule of thumb based on a limited number of case studies (not shown here) is provided hereafter:

\begin{itemize}
	\item $N\approx100$ for very simple AM implementations (1 level of analogy with 2 pressure levels),
	\item $N\approx200$ for a slightly more complex AM (1 level of analogy with 4 pressure levels, or 2 level of analogy with less pressure levels),
	\item $N\approx500$ for significantly more complex AMs (2-3 levels of analogy with 4 pressure levels for the atmospheric circulation, and 2 to 4 levels for the moisture analogy).
\end{itemize}

The influence of the size of the IG (proportion of the total population) selected for mating was also assessed (Fig. \ref{fig:option_popratio_score}). It does not appear that this parameter is critical to the quality of the optimizations, provided it is not too big. A value of 50~\% seems a wise choice.


\section{Recommended configuration of GAs}
\label{sec:recommendations}

Optimizations by GAs for AMs of varying complexities were performed with a large number of combinations of operators in order to make recommendations for optimizing AMs. The conclusions are:

\begin{itemize}
	\item The population size should be in accordance with the complexity of the AM to optimize: from 100 for the simple ones, up to 500 for the most complex AMs.
	
	\item The value of the ratio for the IG is not so important, and value of 50\% seems quite appropriate.
	
	\item Ratio-elitism is slightly better than tournaments for the natural selection operator, but it is not decisive.
	
	\item The performance of the operators for the couples selection perform relatively similarly. The roulette wheel weighting and the tournament selection are more efficient in terms of convergence and required number of evaluations.
	
	\item Most crossover operators have relatively similar performance. Binary-like crossover with two points of intersection are better than others, especially for convergence.
	
	\item Mutation has a clearly dominant influence. Three mutation operators stand out: the non-uniform mutation, the multi-scale mutation, and the chromosome of adaptive search radius. The latter is the most robust as it has no controlling parameter.
	
\end{itemize}

The optimization does not systematically converge to the global optimum (but still often nearby), which is why it is recommended to do several optimizations in parallel in order to compare the results, analyze the convergence, and keep the best. It may be wise to consider the three mutation operators in parallel. In order to be confident in the optimized AMs, we propose using a set of the following mutation operators:

\begin{itemize}
	\setlength\itemsep{-4px}
	\item 1x non-uniform, $p_{mut}=0.05$, $G_{m}=50$, $\omega=0.1$
	\item 1x non-uniform, $p_{mut}=0.05$, $G_{m}=100$, $\omega=0.1$
	\item 1x non-uniform, $p_{mut}=0.1$, $G_{m}=100$, $\omega=0.1$
	\item 1x multi-scale,  $p_{mut}=0.1$
	\item 2x chromosome of adaptive search radius
\end{itemize}

where $p_{mut}$ is the mutation rate (or mutation probability), $G_{m,r}$ is the maximum number of generations during which the magnitude of the research varies, and $\omega$ is a threshold chosen by the user to maintain a minimum search radius when the number of generations $G>G_{m,r}$.


\section{Conclusions}
\label{sec:conclusions}

In order to automatically optimize several AM variants and to get rid of the limitations of the usual sequential calibration, GAs were evaluated. Given the large number of existing operators and options, multiple variants were assessed systematically in order to identify which operators are important, and which variants work best for the considered AM implementations. The mutation operator was identified as a key element for this application, and new variants that proved efficient were provided, such as the chromosome of adaptive search radius that is very robust (no control parameter). Recommendations were established for a relevant use of GAs for the optimization of AMs. If using GAs to optimize AMs is computationally intensive, once an AM is calibrated, its use in real-time operations it very fast and lightweight.

It is not excluded that another global optimization method or other operators of GAs may perform still better. However, the relevance of such an approach has now been proved as it results in parameters of AMs that are relevant and fully automatically, globally and objectively established. A global optimization is the only way to take into account all the dependencies between parameters and levels of analogy.

The global optimization approach allows easily adapting AMs to new regions by potentially taking into account local meteorological influences, and has thus a great potential of use. Moreover, it allows exploring automatically datasets in order to extract the most relevant variables. It is thus possible to try assessing other predictands, such as the temperature, the limit of snowfall, the occurrence of hail, or wind, while leaving the algorithms select the best variables and the associated parameters.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\acknowledgments
Thanks to Hamid Hussain-Khan of the University of Lausanne for his help and availability, and for the intensive use of the cluster he is in charge of. Thanks to Dominique B\'{e}rod for his support and to Michel Bierlaire for his advices on optimization methods.

Thanks to the Roads and Water courses Service, Energy and Water Power Service of the Wallis Canton, the Water, Land and Sanitation Service of the Vaud Canton, and the Swiss Federal Office for Environment (FOEV) who financed the MINERVE (Mod\'{e}lisation des Intemp\'{e}ries de Nature Extr\^{e}me des Rivi\`{e}res Valaisannes et de leurs Effets) project which started this research. The fruitful collaboration with the Laboratoire d'Etude des Transferts en Hydrologie et Environnement of the Grenoble Institute of Technology (G-INP) was made possible thanks to the Herbette Foundation. NCEP reanalysis data provided by the NOAA/OAR/ESRL PSD, Boulder, Colorado, USA, from their Web site at http://www.esrl.noaa.gov/psd/. Precipitation time series provided by MeteoSwiss. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% APPENDIXES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Use \appendix if there is only one appendix.
%\appendix

% Use \appendix[A], \appendix}[B], if you have multiple appendixes.
%\appendix[A]

%% Appendix title is necessary! For appendix title:
%\appendixtitle{}

%%% Appendix section numbering (note, skip \section and begin with \subsection)
% \subsection{First primary heading}

% \subsubsection{First secondary heading}

% \paragraph{First tertiary heading}

%% Important!
%\appendcaption{<appendix letter and number>}{<caption>} 
%must be used for figures and tables in appendixes, e.g.,
%
%\begin{figure}
%\noindent\includegraphics[width=19pc,angle=0]{figure01.pdf}\\
%\appendcaption{A1}{Caption here.}
%\end{figure}
%
% All appendix figures/tables should be placed in order AFTER the main figures/tables, i.e., tables, appendix tables, figures, appendix figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make your BibTeX bibliography by using these commands:
\bibliographystyle{ametsoc2014}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TABLES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Enter tables at the end of the document, before figures.
%%
%

\begin{table}[t]
	\caption{Assessed operator for couples selection.}
	\begin{center}
		\begin{tabular}{ll}
			\hline\hline  & \textbf{Couples selection operators} \\ 
			\hline 
			A & Rank pairing \\ 
			B & Random pairing \\ 
			C & Roulette wheel weighting on rank \\ 
			D & Roulette wheel weighting on fitness \\ 
			E & Tournament selection (3 candidates) \\ 
			F & Tournament selection (4 candidates) \\ 
			\hline 
		\end{tabular}
	\end{center}
	\label{tab:assessed_couples_selection_operators}
\end{table}

\begin{table}[t]
	\caption{Assessed operators for chromosome crossover.}
	\begin{center}
		\begin{tabular}{ll}
			\hline\hline  & \textbf{Chromosome crossover operators} \\ 
			\hline 
			1 & Single-point crossover \\
			2 & Two-point crossover \\
			3 & Multiple-point crossover (3 points) \\
			4 & Multiple-point crossover (5 points) \\
			5 & Uniform crossover \\
			6 & Blending method (2 points, unshared $\beta$) \\
			7 & Blending method (4 points, unshared $\beta$) \\
			8 & Blending method (2 points, shared $\beta$) \\
			9 & Blending method (4 points, shared $\beta$) \\
			10 & Linear crossover (2 points) \\
			11 & Linear crossover (4 points) \\
			12 & Heuristic crossover (2 points, unshared $\beta$) \\
			13 & Heuristic crossover (4 points, unshared $\beta$) \\
			14 & Heuristic crossover (2 points, shared $\beta$) \\
			15 & Heuristic crossover (4 points, shared $\beta$) \\
			16 & Binary-like crossover (2 points, unshared $\beta$) \\
			17 & Binary-like crossover (4 points, unshared $\beta$) \\
			18 & Binary-like crossover (2 points, shared $\beta$) \\
			19 & Binary-like crossover (4 points, shared $\beta$) \\
			20 & Linear interpolation \\
			21 & Free interpolation \\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:assessed_crossover_operators}
\end{table}

\begin{table}[t]
	\caption{Assessed mutation operators with the number of variants considered (combination of parameters).}
	\begin{center}
		\begin{tabular}{llc}
			\hline\hline  & \textbf{Mutation operator} & \textbf{Variants}\\ 
			\hline 
			1 & Uniform mutation & 3 \\
			2 & Variable uniform mutation & 27 \\
			3 & Constant normal mutation & 9 \\
			4 & Variable normal mutation & 36 \\
			5 & Non-uniform mutation & 27 \\
			6 & Individual adaptive mutation rate & 1 \\
			7 & Individual adaptive search radius & 1 \\
			8 & Chromosome of adaptive mutation rate & 1 \\
			9 & Chromosome of adaptive search radius & 1 \\
			10 & Mutli-scale mutation & 3 \\
			11 & No mutation & 1 \\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:assessed_mutation_operators}
\end{table}

\begin{table}[t]
	\caption{Further assessments of mutation operators.}
	\begin{center}
		\begin{tabular}{llrrr}
			\hline\hline  & \textbf{Mutation operator} & \textbf{$p_{mut}$} & \textbf{$G_{max}$} & \textbf{$\omega$}\\ 
			\hline 1 & Non-uniform mutation & 0.01 & 50 & 0.1 \\
			2 & Non-uniform mutation & 0.05 & 50 & 0.1 \\
			3 & Non-uniform mutation & 0.1 & 50 & 0.1 \\
			4 & Non-uniform mutation & 0.2 & 50 & 0.1 \\
			5 & Non-uniform mutation & 0.4 & 50 & 0.1 \\
			6 & Non-uniform mutation & 0.01 & 100 & 0.1 \\
			7 & Non-uniform mutation & 0.05 & 100 & 0.1 \\
			8 & Non-uniform mutation & 0.1 & 100 & 0.1 \\
			9 & Non-uniform mutation & 0.2 & 100 & 0.1 \\
			10 & Non-uniform mutation & 0.4 & 100 & 0.1 \\
			11 & Mutli-scale mutation &  0.01 &&\\
			12 & Mutli-scale mutation &  0.05 && \\
			13 & Mutli-scale mutation &  0.1 && \\
			14 & Mutli-scale mutation &  0.2 && \\
			15 & Mutli-scale mutation &  0.4 && \\
			16 & \multicolumn{4}{l}{Chromosome of adaptive search radius} \\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:assessed_mutation_operators_bis}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Enter figures at the end of the document, after tables.
%%
%


\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{fig01.pdf}\\
	\end{center}
	\caption{Genetic Algorithms operational flowchart.}
	\label{fig:structure_gas}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{fig02.pdf}\\
	\end{center}
	\caption{Evolution of the score of the best individuals over generations for the 10 optimizations processed for a given configuration. The continuous bottom line represents the score of the sequential approach and the dashed one (top), the supposed global optimum. The circles represent the end of the optimization (when the best individual did not progress during 20 generations).}
	\label{fig:gas_evolution_good}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{fig03.pdf}\\
	\end{center}
	\caption{Same as Fig. \ref{fig:gas_evolution_good}, but for a GAs configuration considered as less relevant.}
	\label{fig:gas_evolution_bad}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{fig04.pdf}\\
	\end{center}
	\caption{Influence of the couples selection operators (Table \ref{tab:assessed_couples_selection_operators}) on the optimization performance (improvement of the score). The box extends from the lower to upper quartile values of the data, with a line at the median. The whiskers extend from the box to 1.5 times the interquartile range. Flier points are those past the end of the whiskers. The star represents the median. The gray box highlights the best options.}
	\label{fig:operator_selectcoupl_score}
\end{figure}

\begin{figure*}[t]
	\begin{center}
		\noindent\includegraphics[width=39pc,angle=0]{fig05.pdf}\\
	\end{center}
	\caption{Influence of the chromosome crossover operators (Table \ref{tab:assessed_crossover_operators}) on the optimization performance (improvement of the score). Same conventions as Fig. \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_crossover_score}
\end{figure*}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=19pc,angle=0]{fig06.pdf}\\
	\end{center}
	\caption{Influence of the mutation operators (Table \ref{tab:assessed_mutation_operators}) on the optimization performance. Same conventions as Fig. \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_mutation_score}
\end{figure}

\begin{figure*}[t]
	\begin{center}
		\noindent\includegraphics[width=33pc,angle=0]{fig07.pdf}\\
	\end{center}
	\caption{Influence of the mutation operators (Table \ref{tab:assessed_mutation_operators_bis}) on the optimization performance, leaving the optimizer choose the pressure level of the atmospheric circulation analogy (single level of analogy). The predictand is precipitation over a subcatchment in the Swiss Alps (Binn-Simplon region). The continuous bottom line represents the score of the sequential calibration and the dashed superior line, the score of the optimization without automatic selection of the pressure levels. Same conventions as Fig. \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_mutation_score_atmlevel}
\end{figure*}

\begin{figure*}[t]
	\begin{center}
		\noindent\includegraphics[width=33pc,angle=0]{fig08.pdf}\\
	\end{center}
	\caption{Same as Fig. \ref{fig:operator_mutation_score_atmlevel}, but for another region in the Swiss Alps, with different atmospheric influences.}
	\label{fig:operator_mutation_score_rhoneamont}
\end{figure*}

\begin{figure*}[t]
	\begin{center}
		\noindent\includegraphics[width=33pc,angle=0]{fig09.pdf}\\
	\end{center}
	\caption{Same as Fig. \ref{fig:operator_mutation_score_atmlevel}, but with a second level of analogy on moisture variables.}
	\label{fig:operator_mutation_score_r2}
\end{figure*}

\begin{figure*}[t]
	\begin{center}
		\noindent\includegraphics[width=33pc,angle=0]{fig10.pdf}\\
	\end{center}
	\caption{Same as Fig. \ref{fig:operator_mutation_score_r2}, but with a preselection on air temperature rather than a fixed calendar window.}
	\label{fig:operator_mutation_score_r4}
\end{figure*}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=11pc,angle=0]{fig11.pdf}\\
	\end{center}
	\caption{Influence of the natural selection operators on the optimization performance. Same conventions as Fig. \ref{fig:operator_selectcoupl_score}.}
	\label{fig:operator_selectnat_score}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=11pc,angle=0]{fig12.pdf}\\
	\end{center}
	\caption{Influence of the population size on the optimization performance. Same conventions as Fig. \ref{fig:operator_selectcoupl_score}.}
	\label{fig:option_taillepop_score}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\noindent\includegraphics[width=11pc,angle=0]{fig13.pdf}\\
	\end{center}
	\caption{Influence of the intermediate population (IG) ratio on the optimization performance. Same conventions as Fig. \ref{fig:operator_selectcoupl_score}.}
	\label{fig:option_popratio_score}
\end{figure}



\end{document}