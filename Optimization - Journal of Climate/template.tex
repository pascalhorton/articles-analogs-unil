%% Version 4.3.2, 25 August 2014
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template.tex --  LaTeX-based template for submissions to the 
% American Meteorological Society
%
% Template developed by Amy Hendrickson, 2013, TeXnology Inc., 
% amyh@texnology.com, http://www.texnology.com
% following earlier work by Brian Papa, American Meteorological Society
%
% Email questions to latex@ametsoc.org.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Start with one of the following:
% DOUBLE-SPACED VERSION FOR SUBMISSION TO THE AMS
%\documentclass{ametsoc}


% TWO-COLUMN JOURNAL PAGE LAYOUT---FOR AUTHOR USE ONLY
\documentclass[twocol]{ametsoc}

\usepackage{multirow}
\usepackage{gensymb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% To be entered only if twocol option is used

\journal{jcli}

%  Please choose a journal abbreviation to use above from the following list:
% 
%   jamc     (Journal of Applied Meteorology and Climatology)
%   jtech     (Journal of Atmospheric and Oceanic Technology)
%   jhm      (Journal of Hydrometeorology)
%   jpo     (Journal of Physical Oceanography)
%   jas      (Journal of Atmospheric Sciences)	
%   jcli      (Journal of Climate)
%   mwr      (Monthly Weather Review)
%   wcas      (Weather, Climate, and Society)
%   waf       (Weather and Forecasting)
%   bams (Bulletin of the American Meteorological Society)
%   ei    (Earth Interactions)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Citations should be of the form ``author year''  not ``author, year''
\bibpunct{(}{)}{;}{a}{}{,}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% To be entered by author:

%% May use \\ to break lines in title:

\title{Global Optimization of the Analogue Method by Means of Genetic Algorithms}

%%% Enter authors' names, as you see in this example:
%%% Use \correspondingauthor{} and \thanks{Current Affiliation:...}
%%% immediately following the appropriate author.
%%%
%%% Note that the \correspondingauthor{} command is NECESSARY.
%%% The \thanks{} commands are OPTIONAL.

    %\authors{Author One\correspondingauthor{Author One, 
    % American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}
% and Author Two\thanks{Current affiliation: American Meteorological Society, 
    % 45 Beacon St., Boston, MA 02108.}}

\authors{Pascal Horton\correspondingauthor{Terranum, Rue de l'industrie 35 bis, 1030 Bussigny-pr\`{e}s-Lausanne, Switzerland.}}

%% Follow this form:
    % \affiliation{American Meteorological Society, 
    % Boston, Massachusetts.}

\affiliation{University of Lausanne, and Terranum,  Bussigny-pr\`{e}s-Lausanne, Switzerland}

%% Follow this form:
    %\email{latex@ametsoc.org}

\email{pascal.horton@terranum.ch}

%% If appropriate, add additional authors, different affiliations:
    %\extraauthor{Extra Author}
    %\extraaffil{Affiliation, City, State/Province, Country}

\extraauthor{Michel Jaboyedoff}
\extraaffil{University of Lausanne, Lausanne, Switzerland}

%% May repeat for a additional authors/affiliations:

%\extraauthor{}
%\extraaffil{}

\extraauthor{Charles Obled}
\extraaffil{Universit\'{e} de Grenoble-Alpes, LTHE, Grenoble, France}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%
% Enter your abstract here
% Abstracts should not exceed 250 words in length!
%
% For BAMS authors only: If your article requires a Capsule Summary, please place the capsule text at the end of your abstract
% and identify it as the capsule. Example: This is the end of the abstract. (Capsule Summary) This is the capsule summary. 

\abstract{Enter the text of your abstract here.}

\begin{document}

%% Necessary!
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MAIN BODY OF PAPER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%% In all cases, if there is only one entry of this type within
%% the higher level heading, use the star form: 
%%
% \section{Section title}
% \subsection*{subsection}
% text...
% \section{Section title}

%vs

% \section{Section title}
% \subsection{subsection one}
% text...
% \subsection{subsection two}
% \section{Section title}

%%%
% \section{First primary heading}

% \subsection{First secondary heading}

% \subsubsection{First tertiary heading}

% \paragraph{First quaternary heading}


\section{Introduction}
...
\cite{Horton2012a}


\section{Calibration of the analogue method}
...

\subsection{Data}
\label{section_data}

The analogue method relies on two types of data: predictors, that are atmospheric variables describing the state of the atmosphere at a synoptic scale, and the predictand, which is the local weather time series we want to forecast.

Predictors are generally reanalysis datasets (and outputs of a global numerical weather prediction models for the target situation in operational forecasting, which is not the topic of this paper). We will work here with the NCEP/NCAR reanalysis \citep[6-hourly, 17 atmospheric levels at a resolution of 2.5\degree, see][]{Kalnay1996}, but it can be any other reanalysis.

The predictand (which is to be predicted) is here the daily precipitation (6~a.m. to 6~a.m. the next day) measured at the MeteoSwiss' stations network, for the period 1961-2008. The time series from every available gauging station were averaged over subregions in order to smooth local effects \citep{Obled2002, Marty2012}.


\subsection{The analogue method}
\label{section_analog_method}

The analogue method is a downscaling technique based on the idea expressed by \citet{Lorenz1969}. It aims at forecasting a predictand, often the daily precipitation, on the basis of predictor variables describing the synoptic atmospheric circulation. Its main hypothesis is that similar situations in terms of atmospheric circulation are likely to lead to similar local weather \citep{Bontron2005}.

Multiple variations of the methods are possible, and some aspects and parameters will not be detailed hereafter. There are mainly 2 implementations that are used most often: one that relies on an analogy of the atmospheric circulation, and another that adds a second level of analogy on humidity variables \citep{Obled2002, Bontron2005, Marty2012}.

The method based on the analogy of the synoptic circulation consists in the following steps (Table \ref{table_params_R1}): For a target date, we evaluate the similarity of the atmospheric circulation with every day of the archive by processing the S1 criteria \citep[Eq.\ (\ref{eq:S1}), ][]{Teweles1954, Drosdowsky2003}, which is a comparison of gradients, over a certain spatial window. \citet{Bontron2005} showed that the geopotential heights at 500~hPa and 1000~hPa are the best first predictors of the NCEP/NCAR reanalysis dataset, and that the S1 criteria performs better than scores based on absolute distances. The reason for such better results is that the S1 criteria allows comparing the circulation pattern, by means of the gradients, rather than the absolute value of the geopotential heights. To cope with seasonal effects, candidate data are extracted for every year from the archive within a period of 4 months centred around the target date.

\begin{equation}
\label{eq:S1}
S1=100 \frac {\displaystyle \sum_{i} \vert \Delta\hat{z}_{i} - \Delta z_{i} \vert}
{\displaystyle \sum_{i} max\left\lbrace \vert \Delta\hat{z}_{i} \vert , \vert \Delta z_{i} \vert \right\rbrace }
\end{equation}
where $\Delta \hat{z}_{i}$ is the forecast geopotential height difference between the \textit{i}th pair of adjacent points in the target situation, and $\Delta z_{i}$ is the corresponding observed geopotential height difference in the candidate situation. The differences are processed separately in both directions. The smaller the S1 values are, the more similar the pressure fields.

The $N_{1}$ dates with the lowest values of S1 are considered as analogues to the target day. The number of analogues, $N_{1}$, is a parameter to calibrate. It has an optimum clearly identifiable that is often around 50 dates when we consider only one level of analogy.

Then, the daily observed precipitation amount of the $N_{1}$ resulting dates provide the empirical conditional distribution considered as the probabilistic forecast for the target day.

The other most know method adds a second level of analogy on humidity variables (Table \ref{table_params_R2}). The predictor that \citet{Bontron2004} found as optimal for the France territory is a humidity index made of the multiplication of the precipitable water with the relative humidity at 850~hPa. \cite{Horton2012a} confirmed that this product is better than any other variable from the NCEP/NCAR reanalysis considered independently. When adding a second level of analogy, we subsample $N_{2}$ (30) dates in the $N_{1}$ analogues on the atmospheric circulation, to end up with a smaller number of analogue situations. When we add a second level of analogy, we keep a higher number of analogues on the first level (70 instead of 50).


\subsection{Calibration framework}

The calibration of the analogue method is usually done in a perfect prognosis \citep{Klein1959} framework \citep{BenDaoud2010, Bontron2004}. Perfect prognosis uses observed or reanalyzed data to calibrate the relationship between predictors and predictands. Then, when used in operational forecasting, this relationship is applied to global model forecasts, that contains larger uncertainties. This framework allow us to identify relationships that are as close as possible to the natural links between predictors and predictands, by reducing uncertainties related to numerical forecasting models. However, no model is perfect, and even reanalysis data contains a bias that cannot be ignored. For this reason, the statistical relationships identified in the perfect prognosis framework should be applied to model outputs that are as similar as possible to the model used to elaborate the reanalysis. 

Another reason for working in a perfect prognosis framework is that numerical models evolve continuously, and so does the forecast they provide. Re-forecasts allows us to work on a homogeneous dataset, as they are regularly reprocessed. However, it would involve that we need to redo the calibration procedure every time a new version is available, in order to reduce the bias \citep{Wilson2002}. Moreover, the reforecasts are not re-processed for every new version of the model, meaning we still end up with a bias between the forecast and the archive. Finally, the length of reanalyses datasets are usually much longer than reforecast datasets, which allows us to identify more robust relationships. The size of the archive is indeed an important criteria for the analogue method.

The statistical relationship is established on a calibration period that is as long as possible. For every day of this period, a search for analogues is processed, the precipitation data are associated with the corresponding dates and a forecast score is calculated. During the search for analogues situations, 120 days around the target date are excluded (thus excluding data in the same year) in order to consider only truly independent candidates days.

A validation period is always considered. It consists of an independent period that is never used as target neither candidate date. Validating the parameters of the analogue method is very important in order to avoid over-parametrization and thus to ensure that the statistical relationship is valid on another period.

The accuracy of the parameters is evaluated by means of the CRPS \citep[Continuous Ranked Probability Score,][]{Brown1974, Matheson1976, Hersbach2000}. Let the precipitation variable be denoted $x$ with $x^{0}$ the observed value, and $F(x)$ the predicted cumulative distribution functions (cdf). The mean CRPS of a forecast series of length $n$ can be written:

\begin{equation}
\label{eq:CRPS}
CRPS = \frac{1}{n} \sum_{i=1}^{n} \left(  \int_{-\infty}^{+\infty} \left[ F_{i}(x)-H_{i}(x-x_{i}^{0})\right]^{2} dx \right) 
\end{equation}
where $H(x-x_{i}^{0})$ is the Heaviside function that is null when $x-x_{i}^{0}<0$, and has the value 1 otherwise.

The mean CRPS is processed on the calibration, respectively the validation periods. It means that we average the scores on all days, may they be dry, slightly rainy or with heavy precipitation. There is actually a weighting by the number: climatologically, non-rainy days are often more frequent (of course, depending on the location) and it is essential to forecast them well. The days with heavy precipitation are rare, but can instead provide individual scores ($CRPS_{j}$) very penalizing, which implies that the optimization will also try to forecast them accurately. However, these aspects are not explicitly controlled.


\subsection{The classic calibration approach}

The calibration procedure that we call ''classic'' was developed by \citet{Bontron2004} at the LTHE laborytory (INPG, Grenoble). It determines the optimal settings for the different variables of each level of analogy. The analogy levels (eg the atmospheric circulation or humidity variables) are calibrated sequentially. The procedure consists of the following steps \citet{Bontron2004}:

\begin{enumerate}
	\item Manual choice of the following parameters:
	\begin{enumerate}
		\item meteorological variable,
		\item atmospheric level,
		\item time frame (hour of observation),
		\item initial analogue numbers.
	\end{enumerate}
	
	\item For every level of analogy:
	\begin{enumerate}
		\item Identification of the most skilled unitary cell (1 point for humidity variables and 4 for the geopotential fields when using the S1 criteria) over a large domain. Every point (or cell) of the full domain is assessed jointly on every predictor of the level of analogy (consisting generally of the same variable, but on different atmospheric levels and at different hours).
		\item From this most skilled point, the spatial window is expanded by successive iterations in the direction of greater performance gain. The detailed stages are the followings: (i) The unitary spatial window is expanded in every 4 directions successively. The performance score is processed for these 4 windows. (ii) Only the direction providing the best improvement is applied to our spatial window. (iii) From this new spatial window, an increase in every 4 directions is once again assessed, and the best improvement is applied. (iv) The spatial window grows up by repeating the previous steps, until no improvement is reached.
		\item The number of analogues is optimized for the current level of analogy.
		\item A new level of analogy can be added, based on other variables on predefined atmospheric levels and time frames. The analogue number for the next level of analogy is initiated at a chosen value. Then, the procedure starts again from step (a). The parameters calibrated on the previous analogue levels are fixed and do not change (except the number of analogues, at the final stage). 
	\end{enumerate}
	\item Finally, the numbers of analogues are re-assessed for the different analogue levels. This is done iteratively by varying the number of analogues of each level in a systematic way.
\end{enumerate}

Calibration is done in stages, to determine the optimal parameters systematically. The steps are distinct and previously optimized parameters are generally not reassessed. \citet{Bontron2004} notes however that ''\textit{this type of algorithm, which is changing the parameters of a model in a unique path, can lead to the best solution, provided that there is no local optima}''. The advantage of this method is that it is fast, it provides acceptable results, and it has low computing requirements. We added small improvements to this method by allowing the spatial windows to do other moves, such as: (1) increase in 2 simultaneous directions, (2) decrease in 1 or 2 simultaneous directions, (3) expansion or contraction (in every direction), (4) shift of the window (without resizing) in 8 directions (including diagonals), (5) and finally all the moves described above, but with a factor of 2, 3, or more. For example, we try to increase by 2 units in one (or more) direction. This allows to skip one size that may not be optimal.

These supplementary steps often result in spatial windows that are a bit different, but the performance gain is rather marginal (gain of 0.2\%). These methods are available in the open source software AtmoSwing (Analogue Technique MOdel for Statistical Weather forecastING, www.atmoswing.org).


\section{Motivation for a global optimization}

The classic calibration is fast to optimize one spatial window for a given atmospheric level and temporal window. However, it doesn't provide an objective choice of the atmospheric level and temporal window. The only option is to try systematically every combination, which may be acceptable for no more than 2 levels. However, \citet{Horton2012a} showed that additional atmospheric levels may improve the method.

We observed during the calibration of the analogue method that the resulting parameters vary with the initial choices (such as the number of analogues). In addition, the different levels of analogy (on the atmospheric circulation and the humidity variables) are always calibrated sequentially. However, we can not exclude any dependency between them, which could lead us to select other parameters if we calibrate them together. Simultaneous calibration of all parameters has never been undertaken so far. Only a global optimization can be able to optimize all parameters of all analogy levels simultaneously.

When creating the classic calibration procedure, \citet{Bontron2004} was aware of the problem of dependencies between parameters and wrote: '' \textit{We perceive here the combinatorial aspect of our problem: variables and spatial windows are not independent. We will present our results by first searching the best variable [note: e.g. choice of the atmospheric level and the temporal window for the geopotential height] on a chosen spatial window, and next, the best window for the chosen variable. However, even by repeating the process, are we sure to have found the optimal combination?} ''. And later in his work: '' \textit{Our approach, which is again to vary the parameters one by one -- the others being fixed in a more or less arbitrary manner -- may therefore not exactly lead us to the optimal solution} ''. \citet{Bliefernicht2010} has also faced the combinatorial issue of the parameters of the analogue method and concludes that one needs to be an expert to know their respective influence, their sensitivity and their nonlinear interactions. \citet{BenDaoud2010}, when calibrating the analogue method, also stated that '' \textit{the combinatory aspect related to the calibration was found to be too high for all the parameters to be calibrated simultaneously } ''. The configuration of the analogue method resulting from a classic approach is likely to be a local optimum.

The analogue method needs to be adapted to every new region it is applied, because the leading meteorological influences are specific to this region. Even the choice of the atmospheric level and the temporal window should be reconsidered, when not the variable itself. For example, \citet{BenDaoud2010} found the vertical velocity relevant for the great plains in France, when \citet{Horton2012a} found no interest in this variable in an Alpine environment, because the vertical velocity is mainly related to the orographic effect, and was thus already well related to the atmospheric circulation itself. So, when adapting the analogue method to a new region, we should assess systematically every combination of atmospheric levels, time and spatial windows, which is an intensive task. This procedure can be automatized by a global optimization technique.

Our ambition was then to assess the feasibility of an automatic optimization of the analogue method through various techniques. The objective is to find an approach to optimize all parameters simultaneously, and thus be able to identify the global optimum in the parameter space. In addition, it can overcome the systematic manual assessments of certain parameters such as atmospheric levels and time windows. Finally, it can open new perspectives by allowing the addition of new degrees of freedom, such as a weighting of the criteria values between the atmospheric levels, and the consideration of non-overlapping spatial windows between the atmospheric levels.

\citet{Horton2012a} assessed the ability of the \citep{Nelder1965a} method based on a simplex approach. This technique didn't provide satisfying results and failed at identifying the global optimum. Indeed, the optimized parameters didn't converge and many local optimums came out. The parameters space of the analogue method is very complex and not appropriate for a linear optimization technique. 


\subsection{Characterization of the parameters space}
...


\section{Adapting the genetic algorithms}

Genetic algorithms (GAs) come from the world of stochastic optimization, more specifically from metaheuristic approaches. These are stochastic iterative algorithms that behave like search algorithms by exploiting the characteristics of a problem and are particularly suitable for complex parameters spaces.

Genetic algorithms are part of the family of evolutionary algorithms \citet{Back1993c, Schwefel1993}, which get inspiration from some mechanisms of biological evolution, such as reproduction, genetic mutations, chromosomal crossovers, and natural selection. GAs are the most used technique from evolutionary algorithms \citep{Back1993b}, and they are constantly improving \citep{Haupt2004}. However, with time, the different methods of evolutionary algorithms tend to be similar and share many commonalities \citep{Back1996b, Haupt2004}.

The method was developed by \citet{Holland1992b} and was popularized by \citet{Goldberg1989}. Unlike a linear or local optimization, GAs seek the global optimum on a complex surface, theoretically without restrictions, but with no guarantee to reach it.


\subsection{Basic concepts of the genetic algorithms}

GAs mimic the evolution of a population of individuals in a new environment, by applying rules based on natural processes, such as DNA mutation, chromosomes crossover, natural selection, etc. It simulates the fact in the natural environment, the most suitable individuals tend to survive longer, to reproduce more easily, and so to influence coming generations by providing some genes that provide some good performance in a certain domain. Generation after generation, the DNA mixes and the strong genes cumulate in some individuals \citep{Beasley1996a}. Globally, the fitness of the population to its environment increases, while retaining enough variety to not converge too quickly to a local optimum.

Applications of GAs are very diversified as they can handle many parameters of various types \citep{Joines1996a}, even with very complex cost surfaces \citep{Haupt2004}. GAs work remarkably well with intervariable dependences \citep{Haupt2004}. The objective function to optimize (often named fitness function in this context) can be of different types (mathematical function, experimental or numerical modeling). Only the resulting value is used for optimization. Indeed, these algorithms do not require any knowledge of the problem, which can be used as black-box.

By means of the reproduction operator and the natural selection, the GAs focus on the most promising regions of the parameter space \citep{Holland1992b}. Points (parameters sets) are densified in these areas because the strong genes of the best individuals propagate from generation to generation.

Two conditions guarantee in theory the convergence to the global optimum \citep{Zitzler2004a}: (1) Parameters mutations that can allow to explore the entire parameter space, thus ensures that any value can be achieved with a non-zero probability. (2) A rule of elitism ensuring that an optimal solution can not be lost or damaged.

Practically, GAs allow rapidly approaching satisfactory solutions, but they do not provide the optimum solution for sure \citep{Zitzler2004a}. It is indeed mainly a matter of time. When the optimizer gets closer to the global optimum, any new improvement takes more time to appear, and the final adjustment of the parameters is very time consuming \citep{Back1993a}. For problems that require a significant amount of time in order to evaluate the objective function, as in our case, we have to limit the number of generations to get reasonable processing time. Thus, different acceptable solutions can result from one or more optimizations \citep{Holland1992b}. This is a strength and a weakness of GAs: they are very good at exploring complex parameters spaces in order to identify the most promising areas, but they will not necessarily find the best solution with the optimal values of all parameters \citep{Holland1992b}.


\subsection{Structure and operators}

The GAs optimize a population of individuals (parameters sets). Each individual contains a chromosome (parameters of the analogue method in our case). We call gene every parameter that constitutes the chromosome. The parameters to optimize have long been coded in binary form and assembled as strings in the canonical GAs \citep{Goldberg1989}. Encoding and decoding steps were needed to transform the variable from its floating-point representation into its binary representation, and vice versa, which introduce quantification errors \citep{Haupt2004}. According to \citet{Holland1992b}, working with binary chromosomes was supposed to be more efficient \citep{Goldberg1990a, Back1993b}. However, more and more applications use floating-point representations, allowing to avoid the coding and decoding steps and the quantification errors \citep{Haupt2004}, and which also often resulted in a performance improvement \citep{Goldberg1990a}. It is thus now considered that for continuous variables, a floating-point representation is more suited \citep{Michalewicz1996, Herrera1998a, Haupt2004, Back1996b, Gaffney2010a}. 

There are numerous implementation variants of GAs often optimal for a given problem \citep{Hart1991a,Schraudolph1992a}. However, the structure of the method (Figure \ref{figure_structure_gas}) resulting from the work of \citet{Holland1992b} is common to most applications \citep{Back1993b}. The divergences are the operators implementation, through significantly different algorithms, which has an important effect on the results \citep{Gaffney2010a}.

All operators we used and their options, applied to real coding, are described in the following sections. Many other operators exist, but we will only present the ones we evaluated.


\subsubsection{Genesis of the population}

The first step of the optimization is to generate the initial population. A population is a set of $N$ individuals (each of which represents a point in the space of potential solutions, a parameters set of the analogue method in our application) that we are going to make evolve. A generation is the population at a given time. 

A random initialization based on a uniform distribution is the most current version. The size $N$ of the population is often a compromise between the computation time and the quality of the solution. $N$ must allow sufficient sampling of the solutions field \citep{Beasley1996a}, and should thus vary as a function of chromosome size (ie the number of parameters to be optimized). 


\subsubsection{Natural selection}

Natural selection is performed on the basis of the objective function values. The selection allows to only keep a certain part of the population, usually half ($N/2$), which can access the mating pool (intermediate generation with $N_{mp}$ members). If $N_{mp}$ is too high, the reproduction rate is too low, whereas if it is too small, the strong traits of individuals do not have the ability to accumulate in the same chromosome \citep{Haupt2004}. Several techniques exist, such as:

\begin{itemize}
	\item \textbf{$N_{mp}$-elitism} \citep{Michalewicz1996}: the population is sorted according to the value of the objective function and only the better half is preserved. 
		
	\item \textbf{Tournament selection} \citep{Michalewicz1996, Zitzler2004a}: two individuals are randomly selected and fight. The one with the highest score is chosen, but with a certain probability, in order to reduce the selection pressure. This procedure is repeated until the mating pool is full. Individuals can be selected several times, and thus be represented several times in the mating pool.
\end{itemize}


\subsubsection{Selection of the couples}

Individuals of the mating pool can reproduce. It begins with the selection of pairs (the parents). The techniques implemented in this work are the following:


\begin{itemize}
	\item \textbf{Rank pairing}: individuals are gathered in pairs according to their rank (classified on the performance scores). Consecutive ranks are put together (odd rows are associated with even rows). This approach is easy to achieve, but does not look like a natural process.
	
	\item \textbf{Random pairing}: two individuals are randomly selected to form a couple, according to a uniform law.
	
	\item \textbf{Roulette wheel weighting}: the roulette technique refers to gambling. But unlike casino roulette, this one is biased. Each individual is associated with a sector of the wheel with a certain opening angle, which is its probability of selection \citep{Haupt2004}. The probability assigned to the individuals is proportional to their fitness (objective function), so that the most adapted individuals have the greatest probability of reproduction. There are two techniques for weighting the individuals of the mating pool:
	
	\textit{Roulette wheel weighting on rank}: the probability of each individual depends on its rank $n$:
	\begin{equation}
	p_{n}=\dfrac{N_{mp}-n+1}{\sum^{N_{mp}}_{n=1}n}
	\label{equation_mating_rank_weighting}
	\end{equation}
		
	\textit{Roulette wheel weighting on fitness}: the selection probability is calculated based on the value of the objective function. This approach gives more weight to the best individuals when the distribution of scores is wide, while the weight is almost similar when all individuals have approximately the same score \citep{Haupt2004}. The probability $p_{n}$ of each individual is calculated by the equation \ref{equation_mating_score_weighting}:
	\begin{equation}
	p_{n}=\frac{score_{n}-score_{N_{mp}}}{\sum_{n=1}^{N_{mp}} (score_{n}-score_{N_{mp}})}
	\label{equation_mating_score_weighting}
	\end{equation}
	In our application, the last individual ($N_{mp}$) has zero probability of being selected.

	
	\item \textbf{Tournament selection}: This operator is similar to that used in natural selection, but is applied here for the successive selection of each parent. To select a parent, a number of individuals (2 or 3) are randomly picked and the best is kept. This operation is performed twice, once for each partner. This approach imitates the breeding competition in nature \citep{Haupt2004}.
\end{itemize}


\subsubsection{Chromosome crossover}
			
Once the two parents selected for breeding, they combine their chromosomes and produce two children, bringing the number of individuals in the population of $N_{mp}$ back to $N$ (the parents also return back in the total population in order to complement the next generation). The combination of chromosomes is carried out using a crossover operator, thereby generating two offspring having characteristics derived from both parents. Chromosome crossover widens the search space and favours the combination of strong genes, which can result in more suited children. It allows a mixing of genes and accumulation of positive mutations.

The evaluated crossover operators are the following:

\begin{itemize}
	\item \textbf{Single-point crossover}: a crossover point is randomly chosen for the pair. The genes (our parameters) located after that point are exchanged in between the two chromosomes.
	
	\item \textbf{Two-point crossover}: works like the single-point crossover, but there are two intersections defining the segments to exchange. This approach, which significantly extends the search space for the children, is considered more efficient than the previous \citep{Beasley1993a, Haupt2004}.
	
	\item \textbf{Multiple-point crossover} \citep{DeJong1975a}: it is a generalization of the previous, with a number of crossover points up to the number of genes.
	
	\item \textbf{Uniform crossover} \citep{Syswerda1989}: for each gene of the chromosome, it is randomly chosen to exchange or not the values between the parents.
	
	\item \textbf{Binary-like crossover} \citep{Haupt2004}: chromosome crossover on a binary coding can generate new values for variables located at intersection points, since the crossovers are applied at the bit level, thus often within a gene. This is not the case for the floating-point representation, since the crossover is performed between the genes. To reproduce the behaviour present in the original algorithms, which introduces new information, \citet{Haupt2004} propose an operator that combines standard crossover with an interpolation approach. The genes located after a crossover point are exchanged, but the gene located at the intersection is modified as follows (equation \ref{equation_mating_as_binary}):
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = g_{p1,n} - \beta (g_{p1,n} - g_{p2,n}) \\
	g_{o2,n} = g_{p2,n} + \beta (g_{p1,n} - g_{p2,n}) \\
	\end{array} \right.
	\label{equation_mating_as_binary}
	\end{equation}
	where $g_{o1,n}$ and $g_{o2,n}$ are the $n$-th gene of the two new offspring, and $g_{p1,n}$ and $g_{p2,n}$ are those of the two parents. $\beta$ is a random value between 0 and 1.
	
	\item \textbf{Blending method} \citep{Radcliffe1991a}: in this approach, instead of exchanging the genes in between the chromosomes after one or multiple crossover points, these are combined by linear combination (equation \ref{equation_mating_blending_method}). The genes of the parents are blended together using a random value ($\beta$) that can be unique for the whole chromosome, or that can change for every gene. The genes of the offspring are bounded by the genes of the parents, no value can be out of their range.
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = \beta g_{p1,n} + (1-\beta)g_{p2,n} \\ 
	g_{o2,n} = (1-\beta) g_{p1,n} + \beta g_{p2,n} \\
	\end{array} \right.
	\label{equation_mating_blending_method}
	\end{equation}
	
	\item \textbf{Linear crossover} \citep{Wright1991a}: in order to allow the genes to take values outside the interval defined by the parents, a method of extrapolation is necessary. Linear crossover introduces such an approach, and produces three children from two parents, following equation \ref{equation_mating_linear_crossover}. Less couples are required in order to fill up the generation.
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = 0.5 g_{p1,n} + 0.5 g_{p2,n} \\ 
	g_{o2,n} = 1.5 g_{p1,n} - 0.5 g_{p2,n} \\ 
	g_{e3,n} = - 0.5 g_{p1,n} + 1.5 g_{p2,n} \\ 
	\end{array} \right.
	\label{equation_mating_linear_crossover}
	\end{equation}
	
	\item \textbf{Heuristic crossover} \citep{Michalewicz1996}: it is a variation of the latter methods that relies on the following equation:
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	g_{o1,n} = \beta (g_{p1,n} - g_{p2,n}) + g_{p1,n} \\
	g_{o2,n} = \beta (g_{p2,n} - g_{p1,n}) + g_{p2,n} \\
	\end{array} \right.
	\label{equation_mating_heuristic_crossover}
	\end{equation}
	
	\item \textbf{Linear interpolation}: unlike previous techniques, this technique does not rely on crossover points, but on a linear interpolation on every gene of the couple (equation \ref{equation_mating_linear_interpolation}).
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	c_{o1} = c_{p1} - \beta (c_{p1} - c_{p2}) \\
	c_{o2} = c_{p2} + \beta (c_{p1} - c_{p2}) \\
	\end{array} \right.
	\label{equation_mating_linear_interpolation}
	\end{equation}
	where $c_{o1}$ and $c_{o2}$ are the full chromosomes of the offspring, and $c_{p1}$ an $c_{p2}$ are the ones of the parents. As before, $\beta$ is a random value between 0 and 1, and is here the same for every gene.
	
	\item \textbf{Free interpolation}: this technique performs interpolation on each gene, like the previous one; but in this case, the weighting factor changes for each gene:
	\begin{equation}
	\left\lbrace \begin{array}{l} 
	c_{o1} = c_{p1} - [\beta_{1} (g_{p1,1} - g_{p2,1}), \beta_{2} (g_{p1,2}\\
	~~~~~~~~~~~~ - g_{p2,2}), ..., \beta_{Ng} (g_{p1,N_{g}} - g_{p2,N_{g}})] \\
	c_{o2} = c_{p2} + [\beta_{1} (g_{p1,1} - g_{p2,1}), \beta_{2} (g_{p1,2}\\
	~~~~~~~~~~~~ - g_{p2,2}), ..., \beta_{Ng} (g_{p1,N_{g}} - g_{p2,N_{g}})] \\
	\end{array} \right.
	\label{equation_mating_free_interpolation}
	\end{equation}
	where $N_{g}$ is the number of genes, and $\beta$ is here independent between the genes.
	
\end{itemize}

Many other methods or variations exist, combining the advantages of different approaches. The performance of the variants being related to the problem to be addressed, we can not identify a priori the best technique for our application.


\subsubsection{Mutation}

The combination of strong genes by the operator of chromosomes crossover is theoretically the most important operating mechanism in the conventional GAs \citep{Holland1992b,Back1993b}. However, many studies identify the mutation process as main operator, and crossovers as secondary \citep[see][]{Back1992a,Back1996a,Back1996b,Smith1997a,Deb1999,Haupt2004,Costa2005a,Costa2007a}.

The mutation operator is a direct modification of genes. In a binary coding, it is implemented as an inversion of one bit in a chromosome, while in real coding, it is done by changing the value of a gene. Mutations add diversity to the population and prevent a freeze of the evolution, or a genetic drift to a local optimum. Thus, it makes the convergence to the global optimum theoretically possible \citep{Beasley1993a}, as they allow exploring beyond the current region of the variable space. They therefore help prevent the algorithm to converge too quickly to a local optimum and bring new characteristics that were not present in the original population \citep{Haupt2004}. 

The evaluated and developed mutation operators are the following:

\begin{itemize}
	\item \textbf{Uniform mutation}: The mutation rate is constant and equal for every gene of each individual; they all have the same probability to mutate. When a gene is selected for mutation, a new random value is assigned, according to a uniform law.
	
	\item \textbf{Variable uniform mutation} \citep{Fogarty1989}: a variable mutation rate over the generations was first suggested by \citet{Holland1992b} and evaluated by \citet{Fogarty1989}. It improved significantly the performance of GAs. In most applications, the mutation rate decreases with the generations, in a deterministic and global (for all individuals) manner \citep{Back1992b}. Its optimum configuration depends on the size of the chromosomes, of the properties of the objective function, and of the population size \citep{Back1992b}. We implemented this operator according to equation \ref{equation_mutation_uniformvariable}.
	\begin{equation}
	p_{n,G} = p_{G_{0}}+\left( \dfrac{p_{G_{0}}-p_{G_{m,p}}}{G_{m,p}} \right) min\left\lbrace G,G_{m,p}\right\rbrace 
	\label{equation_mutation_uniformvariable}
	\end{equation}
	where $p_{n,G}$ is the mutation rate (probability) of the gene $n$ for generation number $G$, $G_{m,p}$ is the maximum number of generations during which the mutation rate varies. $p_{G_{0}}$ is the initial mutation probability, and $p_{G_{m,p}}$ is the final one. $p_{G_{0}}$, $p_{G_{m,p}}$ and $G_{m,p}$ are the three controlling parameters of the operator. The evolution of the mutation rate is linear.
	
	\item \textbf{Constant normal mutation}: many users use a normal distributions to generate new values. The gene $g$ that mutate becomes:
	\begin{equation}
	g' = N(g,\sigma^{2})
	\label{equation_mutating_normal_distribution}
	\end{equation}
	where $\sigma$ is the standard deviation of the distribution. The disadvantage of this technique is that an accurate value of $\sigma$ must be chosen \citep{Haupt2004}, which is impossible to know beforehand.
	
	\item \textbf{Variable normal mutation}: with the same logic that the variable uniform mutation, we tested a mutation operator using a normal distribution with a variable mutation rate and standard deviation. The mutation rate is calculated with equation \ref{equation_mutation_uniformvariable}. On the same principle, we decrease linearly the standard deviation over the generations:
	\begin{equation}
	\sigma_{n,G} = \sigma_{G_{0}}+\left( \dfrac{\sigma_{G_{0}}-\sigma_{G_{m,\sigma}}}{G_{m,\sigma}} \right) min\left\lbrace G,G_{m,\sigma}\right\rbrace 
	\label{equation_mutation_normalvariable}
	\end{equation}
	where $\sigma_{n,G}$ is the standard deviation of gene $n$ et generation number $G$, $\sigma_{G_{0}}$ is the initial standard deviation, $\sigma_{G_{m,\sigma}}$ is the final standard deviation, $G_{m,\sigma}$ is the maximum number of generations during which the standard deviation varies. $p_{G_{0}}$, $p_{G_{m,p}}$, $G_{m,p}$, $\sigma_{G_{0}}$, $\sigma_{G_{m,\sigma}}$ and $G_{m,\sigma}$ are the six parameters of the method.
	
	\item \textbf{Non-uniform mutation} \citep{Michalewicz1996}: two random numbers are picked based on a uniform law: $r_{1}$, which determines the direction of the change, and $r_{2}$, which determines its magnitude. The new value of the gene is given by the following equation:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} \left(1 - \dfrac{G}{G_{m}} \right)^{2} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} \left(1 - \dfrac{G}{G_{m}} \right)^{2} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_nonuniform_original}
	\end{equation}
	where $a_{n}$ is the is the lower bound of the $n$-th gene, $b_{n}$ its upper bound, $G$ the present generation, and $G_{m}$ the maximum number of generations.
	
	We adapted this operator for our application, which is not based on a predefined number of generations:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} \left(1 - \min \left\lbrace \dfrac{G}{G_{m,r}}, 1 \right\rbrace \left(1-\omega\right) \right)^{2} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} \left(1 - \min \left\lbrace \dfrac{G}{G_{m,r}}, 1 \right\rbrace \left(1-\omega\right) \right)^{2} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_nonuniform}
	\end{equation}
	where $G_{m,r}$ is the maximum number of generations during which the magnitude of the research varies, and $\omega$ is a threshold chosen by the user to maintain a minimum search radius when $G>G_{m,r}$. During the first generations, the exploration extent covers the entire parameters space. However, this area is reduced over generations, allowing exploitation of local solutions.
	
	\item \textbf{Individual adaptive mutation rate} \citep{Back1992a}: based on the ides of Evolution Strategies \citep[see][]{Rechenberg1973, Schwefel1981}, \citet{Back1992a} introduced a concept of self-adaptive genetic algorithms. The idea is to distribute control parameters within individuals themselves, which partially decentralize control of the evolution. It allows reducing the parametrization of GAs and introducing a notion of self-management. The first approach is the introduction of a mutation rate per individual, that mutates itself under its own probability \citep{Back1992a}. Then, the eventual new rate is used to mutate the genes of the individual. Thus, as this rate decreases, it will have less probability of being itself mutated. This approach is close to the natural adaptation phenomena. A population less suited to its environment is changing faster than better adapted population. Mutations are performed according to a constant uniform distribution. The initial mutation rates are randomly chosen \citep{Back1992a} and the method has no parameter. Other approaches exist to introduce a self-adaptation \citep[see][]{Smith1997a,Deb1999,Deb2001a}.
	
	\item \textbf{Individual adaptive search radius} \citep{Horton2012a}: based on the ideas of the non-uniform mutation, we introduce a search radius in the approach of individual adaptive mutation rates. This search radius $r_{a}$, bounded between 0 and 1, is also adaptive and behaves similarly to the adaptive mutation rates. In order to separate its evolution to the one of the mutation rate, its own value is considered initially as a self-mutation rate to eventually mutate before being used as a normalized search radius. The value of a mutated gene is given by the following equation, which is a simplification of the non-uniform mutation:
	\begin{equation}
	g_{n}^{'} = 
	\left\lbrace \begin{array}{l l} 
	g_{n} + \left(b_{n}-g_{n}\right) r_{2} r_{a} & if \; r_{1} < 0.5 \\
	g_{n} - \left(g_{n}-a_{n}\right) r_{2} r_{a} & if \; r_{1} \geq 0.5 \\
	\end{array} \right.
	\label{equation_mutation_rayon_adaptatif}
	\end{equation}
	where $r_{1}$ and $r_{2}$ are randomly selected, in the same way as for the non-uniform mutation. No external parameter is therefore necessary.
	
	\item \textbf{Chromosome of adaptive mutation rate} \citep[\textit{n adaptative mutation rate},][]{Back1992a}: analogously to the individual adaptive mutation rate, this approach leaves the control of the evolution rate to the individuals themselves. The difference here is that each gene has a specific mutation rate. The main advantage is that the tuning of the mutation can be much more precise \citep{Smith1997a}. We therefore consider a second chromosome containing the mutation rate for each gene of the first chromosome. The operations of mutation and self-mutation are similar to the case of the individual adaptive mutation rate, but in a distributed way, within the chromosome. Another difference is that we apply the same crossover operations as those applied to the first chromosome, and this for the same crossing points. Thus, during an exchange of genes, children also inherit the mutation rates specific for each of these genes.
	
	\item \textbf{Chromosome of adaptive search radius} \citep{Horton2012a}: we introduced this operator that combines the operations of the chromosome of adaptive mutation rate to our adaptive search radius approach. Similarly, an individual has 3 chromosomes: the first containing the values to be optimized, the second containing the distributed mutation rate, and the last one, the distributed search radius. Again, no external parameters are required.
	 
	\item \textbf{Multi-scale mutation} \citep{Horton2012a}: finally, we developed another approach, that is also based on the search radius concept. However, the latter is not decreasing with time. Methods based on a reduction of the mutation rate or radius simulate a transition from the exploration phase to the exploitation one. The idea is consistent as long as we are confident that the algorithm will converge towards the global optimum. Indeed, one the algorithm is in the exploitation mode, it is very unlikely to go out of the minima it converges to. We wanted to test an approach that combines both exploration and exploitation during the whole optimization. Thus, we considered the search radius $r_{a}$ of equation \ref{equation_mutation_rayon_adaptatif} as a random value for each individual, but restricted to 4 equiprobable values: 1, 0.5, 0.1, 0.02. The only external parameter is the mutation rate which is fixed.


\end{itemize}

When the gene to mutate is represented by a list of distinct values (eg meteorological variable or analogy criterion), the random choice of a new value is always based on a uniform distribution, without notion of search radius. There is indeed no meaning to use operators based on principles of proximity when the latter does not exist.


\subsubsection{Elitism}

We used a process of elitism on natural selection as well as on mutations. This ensures the survival of the best individual so that we do not lose a better solution. This approach is very common in the field of GAs \citep{Haupt2004}. After the natural selection operator, if the best individual has not been selected, it is copied to the mating pool instead of an individual randomly picked. After mutation, if the best individual has mutated and if its new version has a lower score than the original, the latter is reinserted instead of an individual randomly chosen.


\subsubsection{Ending the optimization}

The convergence check determines whether the solution is acceptable and if the algorithm may stop. The stopping criteria are not often well documented in GAs case studies \citep{Haupt2004}. We chose to stop the optimization if the best individual does not change for $x$ generations. This value should not be too low to allow the algorithm to escape from a local minimum. In addition, the rate of improvement decreases with the progression of the optimization. It is thus common that the best individual does not evolve over generations when we get closer to the global solution. We chose a value of $x=20$ generations.


\subsection{Implementation and constraints}

Some constraints need to be taken into account. For example, when a crossover or a mutation operation resulted in a parameter value standing out of the authorized bounds, it had to be brought back within the limits. Moreover, the parameters are of different nature: some are continuous, such as the weight, some are discretized, such as the analogues number, or the spatial windows, and finally, some are independent elements in an array, such as the choice of the meteorological variable. New values resulting from the optimizer needs to respect the type of data it represents.

Other constraints exist in between the parameters, such as the atmospheric level of the humidity flux that has to be consistent in between the relative humidity and the two wind components. Another example is the weighting of the different atmospheric levels which has to be normalized.

GAs are very computationally intensive because they require many evaluations of the objective function. These assessments are very long in our application, as they require calculating and assessing a forecast for every day of the calibration period. In order to reduce the computation time, we avoid recalculating the score of an individual who has previously been evaluated and that has not changed. We keep the score of each individual living in the selection until it mutates.

The assessment (calculation of the objective function) of each member of the population of a generation is completely independent and can be performed in parallel on different processors of a computer \citep{Haupt2004, Alliot2005}. We implemented this technique and the resulting time savings was very important. In order to perform optimizations for multiple time series, the use of a cluster is a necessity, which our code allows.


\subsection{Recommendations of parametrization}

The GAs parametrization, such as the mutation rate, population size, natural selection options, and so on, is difficult given the high number of existing variants, each developed for a specific problem \citep{Haupt2004, Costa2007a}. This parametrization depends on the objective function, implementation variants, the bounds of variables to be optimized, and performance indicators. Thus, different studies suggest very different parametrization.

A key element of the parametrization of GAs is finding the right balance between exploration and exploitation \citep{Back1992a, Smith1997a}. Exploration is characterized by a relatively high probability to assess the regions of the parameters space that have not yet been visited. This probability must be sufficiently large at the beginning of the optimization, so that the algorithm is capable of identifying the region where the global optimum is located. Exploitation is characterized by a local search in an area of interest, and generally makes small movements. The latter is interesting to refine the results at the end of the optimization.

\citet{DeJong1975a} and \citet{Grefenstette1986} compared different implementations and parametrizations of GAs on functions of varying complexity. They observed that a small population size improves the initial performance, while a large population improves long-term performance; the size of the population should be selected between 50 and 100. They also observed that the ratio of the population to keep for the mating pool is around 50\% (45\% to 60\%).

Values of the mutation rate varies broadly between the studies: from 0.001 \citep{DeJong1975a} to 0.2 \citet{Haupt2004}. \citet{Back1996b} showed that mutation rates higher than the usual ranges are more optimal at the beginning of optimization, allowing further exploration. The combination of a small population and a high mutation rate works best for the first generations \citep{DeJong1975a, Back1996b, Haupt2004}, but as we could observe, it does not guarantee the quality of the final result. Incremental approaches with varying mutation rates are certainly more optimal but more complex to implement \citep{Back1996a, Back1996b}.


\subsubsection{Comparison process and results}

One of our goals being to make recommendations of parametrization for optimization of the analogue method, proceeded systematically. The results are summarized hereafter \citep[see][for the details]{Horton2012a}. We used concepts from the factorial design approach \citep[see eg.][]{Costa2005,Costa2007,Mariano2010}, which is sometimes used for comparative analysis of different parametrizations of GAs. We processed by stages, analysing in details and in a systematic way every variants of the implemented operators, in combination with multiple other options and parameters in order to take into account eventual co-dependencies. 

In order to evaluate a combination of operators/options, we processed 10 optimizations for one parametrization of GAs. The performances were characterized by four indicators:

\begin{itemize}
	\item mean score: average of the final scores of the 10 optimizations,
	\item convergence: the number of optimizations that converged to a supposed global optimum,
	\item number of generations: characterization of the convergence speed,
	\item number of evaluations of the objective function: characterization of the required calculation time (more realistic than the number of generations).
\end{itemize}

This comparison required tens of thousands of optimizations that were performed on a cluster of the University of Lausanne. The results, detailed in \citet{Horton2012a} are synthesized hereafter:

\begin{itemize}
	\item \textbf{Population size}: we found the following ranges to be accurate in average:

	$50<N<100$ for a very simple implementation of the analogue method (1 level of analogy with 2 atmospheric levels),
		
	$N\approx200$ for a bit more complex method (1 level of analogy with 4 atmospheric levels, or 2 level of analogy with less atmospheric levels),
		
	$N\approx500$ for significantly more complex methods (2-3 levels of analogy with 4 atmospheric levels for the atmospheric circulation, and 2 to 4 levels for the analogy humidity),

	We didn't find any improvement with $N>500$, the results were even surprisingly of a lower quality. However, this cannot be generalized and depends on the analogue method to optimize, and supposedly on the characteristics of the processes generating the precipitations in a given region. 
	
	\item \textbf{Natural selection}: this operator has no significant influence, and both tested implementations work fine, with a slightly better performance for the ratio-elitism.
	
	\item \textbf{Selection of couples}: 6 variants of the couples selection were assessed. The performance of these variants are relatively close, both in terms of score, convergence, and number of evaluations. The random pairing performed the most poorly, when the tournament selection with 3 candidates is slightly superior. The roulette wheel weighting is not far behind, but it is less effective in terms of convergence and number of evaluations. This operator has not a significant role in our application. 
	
	\item \textbf{Chromosomes crossover}: we compared 21 different options of the crossover operators. This analysis revealed some slightly better options, some bad ones, and many averages. Among the bad operators, we find first the heuristic crossover, which is also more demanding in number of evaluations, as well as the linear crossover. Binary-like crossovers (especially with 2 points of intersection, whether $\beta$ is shared or not) are significantly better than the others, especially in terms of convergence. The two points crossover is relatively close. Other operators can be considered usable, yet may not be optimal. Once again, this operator is not the key of the GAs parametrization.
	
	\item \textbf{Mutation}: we compared the 10 mutation operators with different options, bringing the number of variations of this operator to 110. We immediately observed that the mutation operator has a very important role on the performance of the optimizations of the analogue method, and that the other reproduction operators seem of secondary importance. This observation is in line with the work of \citet{Back1996a}, who argues for the importance of mutation over reproduction. He even suggests, in opposition with the theory of \citet{Holland1992a}, that chromosomes crossovers have mostly a corrective role of mutation operations. Various studies have also identified the importance of the mutation operator relative to reproduction \citep[see eg.][]{Back1992a, Back1996b, Smith1997a, Deb1999, Haupt2004, Costa2005a, Costa2007a}.
	
	The mutation operators based on a variable normal or uniform law work very poorly and are difficult to configure. We then observe many operators more or less with the same scores and requiring a variable amount of assessments. The convergence analysis allows us to highlight three best operators:
	
	\textit{Non-uniform mutation} \citep{Michalewicz1996}: this operator is good in terms of convergence, mainly when the number of parameters to optimize is rather low. The number of required evaluations, however, can be quite substantial. The main disadvantage of the non-uniform mutation is the complexity of its parametrization, which is difficult to estimate a priori. These parameters must be carefully chosen to be in line with the evolution rate of the population, and are therefore dependent on the problem being treated. We could observe that the $\omega$ coefficient does not influence performance. The role of $G_{m}$ is rather difficult to judge, but does not seem essential. The mutation rate was found to be important. The difficulty is that the optimal value seems to be very case-related. Indeed, by even changing the precipitation time series (ie optimizing for another region), but not the complexity of the analogue method, the optimal mutation rate changes, making it impossible to estimate in advance.
		
	\textit{Chromosome of adaptive search radius} \citep{Horton2012a}: unlike the previous one, this operator is very robust, as it requires no option and is self-managing. It may be sometimes a little bit less efficient for simple problems, but does not require parametrization, which is an important advantage. It is interesting to notice that our insertion of an extra chromosome representing the search radius gives better performance than other self-adaptive operators (such as, eg, the chromosome of adaptive mutation rate).
		
	\textit{Multi-scale mutation} \citep{Horton2012a}: finally, our multi-scale mutation, which also performs pretty well, can as well be seen as fairly robust, since it requires only one parameter, the mutation rate. Our tests seem to indicate that a high mutation rate here is preferable.
	
	It may be wise to perform multiple optimizations and to consider these three operators in parallel in order to obtain results from algorithms that are either sometimes more efficient or more robust. It is interesting to note that the three best techniques incorporate a notion of search distance. It is likely that this notion is the key to these algorithms, for our application, and allows them to initially explore the parameter domain, and then to converge. The search radius in fact directly represents the notion of transition between exploration and exploitation, in our opinion more than a possible evolution of mutation rates.
	
	\item \textbf{Other options}: A ratio of 50\% for the mating pool seems to be a good choice.
	
\end{itemize}


\subsubsection{Recommended parametrization of GAs}

We evaluated the optimization by genetic algorithms on methods of varying complexity, with a large number of combinations of operators to be able to make recommendations for optimizing the analogue method. Our conclusions are:

\begin{itemize}
	\item The optimization does not systematically converge to the global optimum  (but still often nearby), which is why we recommend doing several optimizations in parallel in order to compare the results, analyse the convergence, and keep the best.
	
	\item The population size should be in accordance with the complexity of the method to optimize: from 50 for the simple ones, up to 500 for the most complex methods.
	
	\item The value of the ratio for the intermediate population is not so important, and value of 50\% seems quite appropriate.
	
	\item Ratio-elitism is slightly better than tournaments for the natural selection operator, but it is not decisive.
	
	\item The performance of the operators for the couples selection perform relatively similarly. The roulette wheel weighting and the tournament selection are more efficient in terms of convergence and the required number of evaluations.
	
	\item Most crossover operators have relatively similar performance. Binary-like crossover, especially with two points of intersection, are better than others, especially for convergence.
	 
	\item Mutation has a clearly dominant influence. Three mutation operators stand out, two of which we have developed: the non-uniform mutation, the chromosome of adaptive search radius, and the multi-scale mutation. The latter two are more robust as they have less or even no parameter.
	 
\end{itemize}

In order to make an optimization of the analogue method with genetic algorithms, it may be wise to consider these three mutation operators in parallel. We would then combine algorithms that are sometimes faster to other that are more robust. In order to be confident in the optimized methods, we propose using a set of the following mutation operators:

\begin{itemize}
	\setlength\itemsep{-4px}
	\item 1x non-uniform, $p_{mut}=0.1$, $G_{m}=50$, $\omega=0.1$
	\item 1x non-uniform, $p_{mut}=0.1$, $G_{m}=100$, $\omega=0.1$
	\item 1x non-uniform, $p_{mut}=0.2$, $G_{m}=100$, $\omega=0.1$
	\item 1x multi-scale,  $p_{mut}=0.1$
	\item 1x multi-scale,  $p_{mut}=0.2$
	\item 3x chromosome of adaptive search radius
\end{itemize}


\section{Global optimization of the analogue method}

We made different attempts to improve the analogue method by means of genetic algorithms. A summary of the main results are provided here \citep[see][for the details]{Horton2012a}. The analogue method has to be adapted to every new location it is applied. We will focus here on an application to the Swiss Alps. Even though the results may differ from one place to another, the applicability of GAs to the analogue method remains universal.


\subsection{Case study description}

The study area is the alpine upper Rhône catchment in Switzerland (Fig.\ \ref{figure_map}). The altitude ranges from 372 to 4634~m.a.s.l.\ and the area is 5524~km$^{2}$. This region in the target of the MINERVE (Mod\'{e}lisation des Intemp\'{e}ries de Nature Extr\^{e}me sur les Rivi\`{e}res Valaisannes et de leurs Effets) project that aims at providing a real-time flood management on the upper Rh\^{o}ne catchment \citep{GarciaHernandez2009b}. The first global optimizations were part of the MINERVE project \citep{Horton2012, Horton2012a}.

The 48 years period (see section \ref{section_data}) was divided into a calibration period and a validation period. Using data independent from the calibration period to validate our results is very important in order to assess the robustness of the improvements and to avoid overparametrization of the method.

In order to reduce potential bias related to trends in the climate change or to the evolution in measurement techniques, the selection of the validation period is evenly distributed over the entire series. Thus, we select one year every six years for validation, which represents a total of eight years for the validation and forty for calibration. The choice of the sequence was made in order to have similar statistical characteristics between the calibration and the validation periods.


\subsection{Optimization of the analogy of atmospheric circulation}

We first optimized the analogy of atmospheric circulation alone, without humidity variables. We started from the most simple method, and increased the complexity of the method, thus the number of degrees of freedom in order to identify the ones that are of particular interest. Thus, the tested parametrization evolved iteratively in complexity. We do not provide the detailed results of the intermediate stages \citep[see][for the details]{Horton2012a}, but only the main conclusions.

We first considered the reference method for the analogy of the atmospheric circulation, based on the 500 and 1000~hPa levels. We let the optimizer choose the number of analogues, both spatial windows with no overlapping constraint, as well as the temporal window (hours of observation of the geopotential). The classic calibration cannot automatically choose the temporal window, neither handle non-overlapping spatial windows, and finally it cannot optimize all parameters simultaneously. With these improvements, we could gain a relative improvement of the CRPSS of 3.97\% and 2.45\% in average for the calibration and the validation periods respectively. Some tests showed that most of the gains are due to the non-overlapping spatial windows.

Then, we provided an additional degree of freedom to the AGs by letting them choose the atmospheric levels along with the other parameters. Evaluation of atmospheric levels is also a non-automated process in the classic calibration. However, we could observe that the choice of this degree of freedom improves the optimization time and may decrease the number of simulations which converge to a single solution. However, most solutions are very close in terms of score. The averaged relative improvement of the CRPSS is 5.63\% for the calibration and 3.82\% for the validation period. The atmospheric levels that were chosen are 500~hPa or 700~hPa for the upper level, and 925~hPa or 1000~hPa (most often) for the lower level.

Parallel analyses showed that the analogy of circulation is incomplete, and that the geopotential still contains relevant data that can improve the statistical relationship. So we added a third predictor that the optimizer can use along with the previous parameters. There is no constraint on the predictors, so that the same atmospheric level can be selected multiple times. Some improvements were found on the score, both for the calibration and the validation periods, confirming that this additional information is beneficial for the quality of the forecast. We then tried with 4 predictors, and so on, up to 8. Every time we added a new predictor to optimize, the score on the calibration period increased, but always in a smaller extent. However, the validation period dropped after 4 predictors, revealing an over-parametrization of the method, and thus a lack of robustness. Moreover, the complexity of the method increases, and its interpretation becomes more and more difficult. We thus found that considering 4 predictors is optimal for our case study, since the gain in CRPSS is significant. It can not be excluded that another number would prevail under other meteorological conditions.

Finally, we added a weighting of these 4 predictors (atmospheric levels), which is also optimized by GAs. The weighting operates in the combination of the S1 criteria of every level, which were previously averaged with equal weights. The role of this new degree of freedom is to give more weight to the levels with greater predictive capacity, and to consider the geopotential variability changes with altitude. 


\subsubsection{Which parameters are optimized ?}

The method we finally optimized for the analogy of the atmospheric circulation, based on 4 levels of the geopotential, is made of the following degrees of freedom:

\begin{itemize}
	\setlength\itemsep{-4px}
	\item the atmospheric levels (4 degrees)
	\item the temporal windows (4)
	\item the spatial windows (16)
	\item the weights (4)
	\item the number of analogues (1).
\end{itemize}

This sums up to 29 degrees of freedom that are optimize simultaneously.

\subsubsection{Results of the optimization of atmospheric circulation}
...

\subsubsection{Discussion}
...


%Nous avons cherché à identifier quelle part du gain est attribuable à quel nouveau degré de liberté par rapport à la méthode M6. De manière générale, nous avons remarqué qu'il y a une forte dépendance entre ces paramètres (fenêtres spatiales, fenêtres temporelles et pondération des niveaux atmosphériques), et qu'il n'est pas recommandable de les considérer séparément. Par exemple, l'introduction des poids (non réoptimisés) dans les paramètres de la méthode M6 a un effet en moyenne nul, mais peut engendrer des pertes non négligeables sur les scores de certains groupements.

%An evaluation of all these parameters with the classic calibration requires multiple assessments of all possible combinations. In addition, these studies are often carried out with a spatial window of fixed size and a fixed number of analogues. So we hope to find solutions as good or better, but automatically.

\subsection{Optimization of the analogy with humidity variables}

We previously worked with a method based on a unique level of analogy. However, we know that humidity variables in a second level of analogy do provide improvements to the method (section \ref{section_analog_method}). So we also want to optimize the humidity index, which is a combination of the relative humidity and the precipitable water. In order to do so, we had to introduce a constraint to the optimizer, so that it selects the same temporal window for both variables. 

We proceeded to an optimization of both levels of analogy simultaneously. This implies that the analogy of the atmospheric circulation may change due to the new humidity information.

When we introduce 2 predictors for the humidity analogy, the number of degrees of freedom raises to 42, and to 54 when we add 4 predictors.



%We tried first to optimize the humidity predictors while keeping the same parameters for the first level of analogy on the atmospheric circulation. The results were not really satisfying, mainly on the scores of the validation. -> POUR FLUX !


\subsubsection{Results of the optimization on both analogy levels}
...

%Ces optimisations ont mené à des paramétrisations différentes de l'analogie de circulation par rapport à précédemment. Il existe donc bien des relations entre les paramètres des différents niveaux d'analogie, soit entre la circulation atmosphérique et le flux d'humidité. Ainsi, en connaissance de l'information du flux d'humidité, la méthode préférera une représentation différente de la circulation. Cet aspect n'a encore jamais été mis en évidence, car les outils habituels de calibration ne le permettent pas. 

%Pour tenir compte des dépendances entre les paramètres des différents niveaux d'analogie, il est nécessaire d'utiliser une méthode d'optimisation globale telle que les AGs.


\subsubsection{Discussion}

The optimization of the analogue method by means of genetic algorithms has been undertaken in stages by releasing progressively new degrees of freedom. This approach allowed us to differentiate contributions to performance gains, as well as to identify possible over-parametrization. The main improvements for our case study are due to the following factors:

\begin{itemize}
	\item Using 4 atmospheric levels for the analogy of circulation. It seems to be the optimal number for the studied region. Beyond that value the validation score drops, revealing a lost in robustness due to over-parametrization.
	\item The automatic and joint optimization of all parameters: the analogues number, the choice of the atmospheric levels and the time windows, and the spatial windows. These parameters are highly interdependent, so we need to optimize them jointly in order to identify optimal combinations. Traditional calibration procedures based on a systematic assessment of every combination is no more possible when considering more than 2 atmospheric levels.
	\item The introduction of distinct spatial windows between atmospheric levels. Indeed, the synoptic circulation is characterized by features with very different scales depending on the height, and important information for predicting rainfall is not necessarily located in the same area from one level to another.
	\item The weighting of the analogy criteria between different atmospheric levels. It can be influenced by the variability of the geopotential with altitude, and the change of some levels significance with the targeted region. 
	\item The joint optimization of the circulation and humidity analogy levels, that are usually calibrated successively. We have been able to demonstrate that there is a dependency between the analogy levels, and that in order to approach the optimal parameters, we must consider them jointly.
\end{itemize}
	
GAs are very useful to optimize complex variants of the analogue method, and to assess new degrees of freedom that were not available so far. However, it can be dangerous to add too many parameters to optimize. Indeed, the optimizer will probably use them to successfully improve the calibration score, but the validation control remains very important in order to determine if we are actually improving the method, or if we are over-parametrizing it.

The convergence of parallel optimizations decreases when the method to optimize becomes more and more complex. The optimizer do not always converge to the exact global optimum, but to its surroundings. This is related to the fact that the optimization slows down when it gets closer to the global optimum, and that we have to stop it before the end due to the required processing time. The resulting parameters may sometimes present significant differences, even though the score is almost similar. Through some Monte-Carlo analyses of the parameter space properties of the analogue method, \citet{Horton2012a} showed that some parameters of the method have a wide range of acceptable values. The spatial windows, for example, can be larger than the optimal size without much impact on the score, while they cannot be smaller \citep[see also][]{Bontron2004}. We could also observe that the choice of the atmospheric level is not a parameter as discrete as we would have thought, and that choosing another level may have reduced consequences on the performance. This is particularly true for higher atmospheric levels and can be more critical for lower layers. It was thus interesting to sometimes get several sets of near-optimal parameters, but with some nuances, in order to have an idea of the sensitivity of the parameters for a given region, and to compare the score on the validation period. In this regard, a cross-validation approach may be recommended.

We tried to optimize the preselection period (the 4-months window) jointly with the other parameters, but we did not get improvement. We also tried optimizing the humidity flux, which is composed of the humidity index multiplied with the wind flux. However, the results were not as good as when we considered the humidity index alone. This may be related to the fact that the optimizer tries to provide the best analogy of the atmospheric circulation in the first place, which makes the wind information less relevant in the second level of analogy.

GAs are relatively heavy to implement and require an IT infrastructure capable of performing thousands hours of calculations. However, they automatically optimize all parameters of the analogue method, what the classic calibration does not allow. We therefore save much human time that was required to assess successively numerous combinations of parameters (particularly the choice of the atmospheric levels and the temporal windows). The ability to optimize jointly all parameters has been important given the strong dependencies between them and between levels of analogy.







	 


% Cet aspect a été confirmé par l'optimisation, qui propose de petites fenêtres pour les niveaux supérieurs, et de grandes fenêtres pour les altitudes proches du sol. Notre hypothèse est que l'information des gradients des couches supérieures au-dessus du bassin suffit à résumer l'orientation et la vitesse des masses d'air d'intérêt, qui sont potentiellement liées à un forçage par-dessus la barrière orographique. De manière complémentaire, les fenêtres plus étendues pour les basses couches cherchent à intégrer la structure de la circulation, car celle-ci renseigne sur les centres actifs et sur le chemin qu'a parcouru la masse d'air éventuellement chargée en humidité avant d'arriver sur la zone d'intérêt.
	
	


\section{Conclusions and perspectives}

We could observe strong dependencies between the parameters of the analogue method. Thus, the classic calibration, which optimizes the parameters successively, may not lead to the optimal combination. Moreover, it contains several manual systematic assessment, such as the choice of the atmospheric levels and the temporal windows.

In order to automatically optimize the analogue method, we evaluated the genetic algorithms, a global optimization technique, that is able to solve complex problems. Given the large number of existing operators and options, we had to evaluate systematically multiple variants in order to identify which operators are important, and which variant works best for the analogue method. We could thus identify that the mutation operator is a key element for our application, and provided new variants that were found to be efficient. Recommendations were established for a relevant use of GAs for the optimization of the analogue method.

GAs provide parametrizations of the analogue method that exceed performance results of classic calibration. In addition, they are used to select the atmospheric levels and the time windows automatically, which was not possible with traditional tools and which can save a considerable amount of human time. A great advantage of a global optimization is its ability to approach or reach optimal parameter values when they are considered jointly. 

We were able to identify that there is a parametric dependence between the analogy of circulation and the moisture humidity. When we consider the two levels together, the optimal parameters of the analogy of circulation are different. This complexity can only be exploited in a suitable manner by global optimization methods.

For our case study, there seems to be an optimum number of atmospheric levels to consider for the analogy of circulation, which is four, before losing performance in validation. We have also been able to improve the analogy of circulation by introducing a weighting between atmospheric levels, and considering independent spatial windows between levels.

However, there is a risk of over-parametrization with this type of approach. It is therefore important to always evaluate the proposed parameters over an independent period for validation. 



%\item Mutation non uniforme: cet opérateur présente potentiellement le meilleur taux de convergence, pour autant qu'il soit bien paramétré. En revanche, si l'évolution de la population n'a pas été correctement estimée au préalable, et que nous choisissons une mauvaise valeur parmi ses trois paramètres, l'optimisation peut être médiocre. Son potentiel est donc prometteur, mais l'opérateur est difficile à maîtriser, car les paramètres sont nombreux et leur valeur dépend du problème à optimiser.
%\item Chromosome de rayons de recherche adaptatifs: nous avons élaboré le chromosome de rayons de recherche adaptatifs en nous inspirant du chromosome de taux de mutation adaptatifs. Notre version est plus performante, car elle intègre une notion de distance dans l'espace des paramètres, qui décroît avec l'évolution de la population, améliorant ainsi la convergence. Une grande force de cet opérateur est qu'il ne nécessite aucun paramètre, il s'autocontrôle et évolue avec la population, ce qui le rend très robuste.
%\item Mutation multi-échelles: la mutation multi-échelles est une nouvelle approche qui ne cherche pas une évolution de l'opérateur de mutation, mais qui effectue des recherches globales, régionales et locales en tout temps. Il est ainsi constamment en train d'explorer l'espace des paramètres complet et d'exploiter l'information locale, les deux aspects importants des algorithmes génétiques. Il ne nécessite qu'un seul paramètre, qui est le taux de mutation. Celui-ci doit cependant être bien choisi, et cette valeur change en fonction de la méthode à optimiser.

% Finalement, les AGs, tels qu'ils ont été implémentés, sont également en mesure de choisir les variables prédicteurs d'intérêt. Si nous n'avons plus vraiment de doute sur le choix des variables pour la prévision des précipitations, en raison des multiples travaux réalisés sur ce sujet, nous ne connaissons pas a priori les variables qui seraient les plus informatives pour prévoir un autre prédictand, comme la température, la grêle ou le vent. Ces outils d'optimisation peuvent alors explorer les variables et nous proposer les meilleures combinaisons. Toutefois, l'automatisation de variables composées (par exemple le flux d'humidité) présentera toujours quelques limites.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\acknowledgments
Thanks to Hamid Hussain-Khan of the University of Lausanne for his help and availability, and for the intensive use of the cluster he is in charge of.

Thanks to the Swiss Federal Office for Environment (FOEV), the Roads and Water courses Service, Energy and Water Power Service of the Wallis Canton and the Water, Land and Sanitation Service of the Vaud Canton who financed the MINERVE project which started this research. NCEP reanalysis data provided by the NOAA/OAR/ESRL PSD, Boulder, Colorado, USA, from their Web site at http://www.esrl.noaa.gov/psd/. Precipitation time series provided by MeteoSwiss. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIXES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Use \appendix if there is only one appendix.
%\appendix

% Use \appendix[A], \appendix}[B], if you have multiple appendixes.
%\appendix[A]

%% Appendix title is necessary! For appendix title:
%\appendixtitle{}

%%% Appendix section numbering (note, skip \section and begin with \subsection)
% \subsection{First primary heading}

% \subsubsection{First secondary heading}

% \paragraph{First tertiary heading}

%% Important!
%\appendcaption{<appendix letter and number>}{<caption>} 
%must be used for figures and tables in appendixes, e.g.,
%
%\begin{figure}
%\noindent\includegraphics[width=19pc,angle=0]{figure01.pdf}\\
%\appendcaption{A1}{Caption here.}
%\end{figure}
%
% All appendix figures/tables should be placed in order AFTER the main figures/tables, i.e., tables, appendix tables, figures, appendix figures.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make your BibTeX bibliography by using these commands:
% \bibliographystyle{ametsoc2014}
% \bibliography{references}

\bibliographystyle{ametsoc2014_no_url}
% \bibliography{references}
\bibliography{../_refs/_articles-optimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Enter tables at the end of the document, before figures.
%%
%
%\begin{table}[t]
%\caption{This is a sample table caption and table layout.  Enter as many tables as
%  necessary at the end of your manuscript. Table from Lorenz (1963).}\label{t1}
%\begin{center}
%\begin{tabular}{ccccrrcrc}
%\hline\hline
%$N$ & $X$ & $Y$ & $Z$\\
%\hline
% 0000 & 0000 & 0010 & 0000 \\
% 0005 & 0004 & 0012 & 0000 \\
% 0010 & 0009 & 0020 & 0000 \\
% 0015 & 0016 & 0036 & 0002 \\
% 0020 & 0030 & 0066 & 0007 \\
% 0025 & 0054 & 0115 & 0024 \\
%\hline
%\end{tabular}
%\end{center}
%\end{table}

\begin{table}[htbp]
	\footnotesize
	\caption{Parameters of the reference method on the atmospheric circulation.}
	\begin{center}
		\begin{tabular}{ccccc}
			\hline \textbf{Level} & \textbf{Variable} & \textbf{Hour} & \textbf{Criteria} & \textbf{Analogues nb} \\ 
			\hline 
			preselection & \multicolumn{4}{c}{$\pm 60$ days around the target date} \\
			\hline 
			\multirow{2}{*}{1} & geopotential hgt 1000~hPa & 12~h & \multirow{2}{*}{S1} & \multirow{2}{*}{50} \\
			& geopotential hgt 500~hPa & 24~h & & \\ 
			\hline 
		\end{tabular} 
	\end{center}
	\label{table_params_R1}
\end{table}


\begin{table}[htbp]
	\footnotesize
	\caption{Parameters of the reference method with humidity variables.}
	\begin{center}
		\begin{tabular}{ccccc}
			\hline \textbf{Level} & \textbf{Variable} & \textbf{Hour} & \textbf{Criteria} & \textbf{Analogues nb} \\ 
			\hline 
			preselection & \multicolumn{4}{c}{$\pm 60$ days around the target date} \\
			\hline 
			\multirow{2}{*}{1} & geopotential hgt 1000~hPa & 12~h & \multirow{2}{*}{S1} & \multirow{2}{*}{70} \\
			& geopotential hgt 500~hPa & 24~h & & \\ 
			\hline
			\multirow{2}{*}{2} & precipitable water * relative humidity 850~hPa & 12~h & \multirow{2}{*}{RMSE} & \multirow{2}{*}{30} \\
			& precipitable water * relative humidity 850~hPa & 24~h & & \\ 
			\hline 
		\end{tabular} 
	\end{center}
	\label{table_params_R2}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Enter figures at the end of the document, after tables.
%%
%
%\begin{figure}[t]
%  \noindent\includegraphics[width=19pc,angle=0]{figure01.pdf}\\
%  \caption{Enter the caption for your figure here.  Repeat as
%  necessary for each of your figures. Figure from \protect\cite{Knutti2008}.}\label{f1}
%\end{figure}


\begin{figure}[htb]
	\centerline{\includegraphics[width=8.3cm]{figure_map.pdf}}
	\caption{Location of the alpine Rhône catchment in Switzerland. (source: Swisstopo)}
	\label{figure_map}
\end{figure}


\begin{figure}[htb]
	\centerline{\includegraphics[width=10.3cm]{figure_structure_gas.pdf}}
	\caption{Genetic algorithms operational flowchart}
	\label{figure_structure_gas}
\end{figure}




\end{document}